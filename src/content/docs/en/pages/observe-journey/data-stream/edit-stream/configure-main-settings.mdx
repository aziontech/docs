---
title: How to configure Data Stream main settings
description: Find out how to configure the main settings of a streaming.
meta_tags: Data Stream, how to, use, stream, event, logs, observe, observability
namespace: documentation_using_dts
permalink: /documentation/products/guides/use-data-stream/
menu_namespace: observeMenu
---

While creating or editing a streaming, there are a few specific main configurations you can define and modify.

To find more detailed information on the product and each configuration, see the [reference documentation](/en/documentation/products/observe/data-stream/).

---

## Via Azion Console

In this section, youâ€™ll modify the main settings of your streaming: name, data source, template, domains, and destination via [Azion Console](https://console.azion.com/).

1. [Access Azion Console](/en/documentation/products/guides/how-to-access-rtm/) > **Data Stream**.
2. From the list, select the streaming you want to edit or click the **Add Streaming** button.
3. Give your streaming a unique and easy-to-remember name.
4. On the **Data Source** dropdown menu, select the one you want to use: **Activity History**, **Edge Applications**, **Edge Functions**, or **WAF Events**.
    - For **Edge Functions** and **WAF Events**, you must be [subscribed to the products](/en/documentation/products/data-stream/first-steps/).
5. On the **Template** dropdown menu, select the one you want to use: a pre-set template according to your chosen data source or a **Custom Template**.
    - If you choose **Custom Template**, add the variables you want to use in your logs' analysis in the **Data Set** code box. See more on [How to create a custom template on Data Stream](/en/documentation/products/guides/data-stream-custom-template/).
6. On **Options**, select between **Filter Domains** or **All Domains**.
    - See more about each option on [How to associate domains on Data Stream](/en/documentation/products/guides/data-stream-associate-domains/).
7. Under **Destination**, select an **Endpoint Type** on the dropdown menu: **Standard HTTP/HTTPS POST**, **Apache Kafka**, **Simples Storage Service (S3)**, **Google BigQuery**, **Elasticsearch**, **Splunk**, **AWS Kinesis Data Firehose**, **Datadog**, **IBM QRadar**, **Azure Monitor**, or **Azure Blob Storage**.
    - You'll see different fields depending on the endpoint type you choose. Find more information on each of them on the [Setting an endpoint page](/en/documentation/products/observe/data-stream/#endpoints).
8. Click the **Save** button.

Watch a video tutorial on how to use Data Stream on Azion's YouTube channel:

<iframe
   src="https://www.youtube.com/embed/CT2VyftnOZs"
   loading="lazy"
   width="600"
   height="400"
   title="How to Use Data Stream"
   frameborder="0"
   allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
   allowfullscreen></iframe>

---

## Via API

For this section, you'll be modifying an existing streaming with an Elasticsearch connector via API. To create a new one, use the [Data Stream POST method](https://api.azion.com/#b9aac0c7-173a-462b-87f6-a18d6558d3e5).

:::tip
Check the [Data Stream Properties](https://api.azion.com/#b9aac0c7-173a-462b-87f6-a18d6558d3e5) table to see all available properties and their accepted values.
:::

1. Run the following `GET` request in your terminal, replacing `[TOKEN VALUE]` with your [personal token](/en/documentation/products/guides/personal-tokens/) to retrieve your `<data_streaming_id>`:

```bash
curl --location 'https://api.azionapi.net/data_streaming/streamings' \
--header 'Accept: application/json; version=3' \
--header 'Authorization: Token [TOKEN VALUE]'
```

2. You'll receive a response with all your existing streamings. Copy the value of the `<id>` that you want to configure.
3. Run a `PATCH` request to modify the streaming as follows:

```bash
curl --location --request PATCH 'https://api.azionapi.net/data_streaming/streamings/<data_streaming_id>' \
--header 'Accept: application/json; version=3' \
--header 'Authorization: Token [TOKEN VALUE]' \
--header 'Content-Type: application/json' \
--data '{
	"name": "My Elasticsearch connector",
    "template_id": 2,
    "domain_ids": [1656613172],
    "data_source": "http",
    "endpoint": {
        "endpoint_type": "elasticsearch",
        "url": "http://elasticsearch-domain.com",
        "api_key": "VuaCfGcBCdbkQm-e5aOx:ui2lp2axTNmsyakw9tvNnw"
    },
    "all_domains": false
}'
```

| Key | Description |
| --- | --- |
| `name` | Name of the streaming |
| `template_id` | Identifier for the template being used |
| `domain_ids` | Array value of the domain's identifiers (integers) you want to associate with the streaming |
| `data_source` | Data source from which you want to stream your data |
| `endpoint` | Endpoint to which you want to stream your data |
| `all_domains` | Used to associate all current and future domains in your account to the streaming |

4. You'll receive a response similar to this:

```json
{
  "results": {
    "id": 1560,
    "name": "My Elasticsearch connector",
    "template_id": 2,
    "data_source": "http",
    "active": true,
    "endpoint": {
      "endpoint_type": "elasticsearch",
      "url": "http://elasticsearch-domain.com",
      "api_key": "VuaCfGcBCdbkQm-e5aOx:ui2lp2axTNmsyakw9tvNnw"
    },
    "all_domains": false
  },
  "schema_version": 3
}
```

Wait a few minutes for the changes to propagate and your streaming will be updated.

:::tip
Check the [Azion API documentation](https://api.azion.com/) and the [OpenAPI specification](https://github.com/aziontech/azionapi-openapi/) to know more about what the Azion API can offer.
:::
<br /><br />
---

import ContributorList from '~/components/ContributorList.astro'

**Contributors** <ContributorList>Contributor</ContributorList>
