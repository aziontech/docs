---
title: Florence-2 Large
description: >-
  Florence 2 Large is an advanced vision foundation model that leverages a prompt-based approach and a massive dataset to excel in various vision and vision-language tasks.
meta_tags: 'edge ai, ai models, artificial intelligence, edge computing'
namespace: docs_edge_ai_models_florence_2
permalink: /documentation/products/ai/edge-ai/models/florence-2/
---

**Florence 2** is an advanced vision foundation model that leverages a prompt-based approach and a massive dataset to excel in various vision and vision-language tasks.

## Model details

| Category | Details |
|----------|---------|
| **Model Name** | Florence 2 |
| **Version** | Large |
| **Model Category** | VLM |
| **Size** | 0.77B parameters |
| **HuggingFace Model** | [microsoft/Florence-2-large](https://huggingface.co/microsoft/Florence-2-large) |
| **OpenAI Compatible endpoint** | [Chat Completions](https://platform.openai.com/docs/api-reference/chat/create) |
| **License** | [MIT](https://huggingface.co/microsoft/Florence-2-large/resolve/main/LICENSE) |

## Capabilities 

| Feature | Details |
|---------|--------|
| Tool Calling | ❌ |
| Azion Long-term Support (LTS) | ❌ |
| Context Length | 1k tokens |
| Supports LoRA | ❌ |
| Input data | Text + Image |

## Usage 

Florence uses tags according to each task it will perform. Below are all the tags with their corresponding tasks.

### Tasks with no additional input:

- Whole image to natural language:

| Tag | Description |
|------|-------------|
| `<CAPTION>` | Image level brief caption |
| `<DETAILED_CAPTION>` | Image level detailed caption |
| `<MORE_DETAILED_CAPTION>` | Image level very detailed caption |

- Whole image or region to text:

| Tag | Description |
|------|-------------|
| `<OCR>` | OCR for entire image |
| `<OCR_WITH_REGION>` | OCR for entire image, with bounding boxes for individual text items |

- Whole image to regions and categories or natural language labels:

| Tag | Description |
|------|-------------|
| `<REGION_PROPOSAL>` | Proposes bounding boxes for salient objects (no labels) |
| `<OD>` | Identifies objects via bounding boxes and gives categorical labels |
| `<DENSE_REGION_CAPTION>` | Identifies objects via bounding boxes and gives natural language labels |

This is an example of how a request for a task with no additional input should look like:

```ts
const modelResponse = await Azion.AI.run("microsoft-florence-2-large", {
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "image_url",
          "image_url": {
            "url": "https://raw.githubusercontent.com/AssemblyAI-Community/florence-2/refs/heads/master/car.png"
          }
        },
        {
          "type": "text",
          "text": "<DETAILED_CAPTION>"
        }
      ]
    }
  ]
})
```

| Property                            | Type | Description                                                         |
| ------------------------------------ | ---- | ------------------------------------------------------------------- |
| `messages`                           | array | An array of message objects in the conversation.                    |
| `messages[].role`                    | string | The role of the message sender.                                     |
| `messages[].content`                 | array | An array of multimodal content elements.                            |
| `messages[].content[].type`          | string | Type of content.                                                    |
| `messages[].content[].image_url`     | object | An object specifying the image URL (only if type is `"image_url"`). |
| `messages[].content[].image_url.url` | string | The actual image URL to be processed.                               |
| `messages[].content[].text`          | string | The textual input content (only if type is `"text"`).               |

Response example:

```json
{
  "id": "chatcmpl-e27716424abf4b3f891ff4850470cb09",
  "object": "chat.completion",
  "created": 1746821581,
  "model": "microsoft-florence-2-large",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "reasoning_content": null,
        "content": "This image contains...",
        "tool_calls": []
      },
      "logprobs": null,
      "finish_reason": "stop",
      "stop_reason": null
    }
  ],
  "usage": {
    "prompt_tokens": 9,
    "total_tokens": 527,
    "completion_tokens": 518,
    "prompt_tokens_details": null
  },
  "prompt_logprobs": null
}
```

| Property                             | Type | Description                                                                 |
| ------------------------------------- | ---- | --------------------------------------------------------------------------- |
| `id`                                  | string | Unique identifier for the completion.                                       |
| `object`                              | string | Type of the returned object.                    |
| `created`                             | number | Unix timestamp of when the completion was created.                          |
| `model`                               | string | Identifier of the model used to generate the response.                      |
| `choices`                             | array | Array of response options returned by the model.                            |
| `choices[].index`                     | number | Index of the current choice in the list.                                    |
| `choices[].message`                   | object | The message object representing the assistant’s reply.                      |
| `choices[].message.role`              | string | Role of the message sender.                       |
| `choices[].message.content`           | array | Main content generated by the assistant.                                    |
| `choices[].message.reasoning_content` | string | Optional reasoning or explanation.                       |
| `choices[].message.tool_calls`        | array | Array of any tool calls made by the assistant.              |
| `choices[].logprobs`                  | object | Log probabilities of the generated tokens. |
| `choices[].finish_reason`             | string | Reason for stopping the generation.                                                |
| `choices[].stop_reason`               | string | More detailed reason for stopping.                          |
| `usage.prompt_tokens`                 | number | Number of tokens in the prompt.                                             |
| `usage.completion_tokens`             | number | Number of tokens in the generated completion.                               |
| `usage.total_tokens`                  | number | Total number of tokens consumed.                                            |
| `usage.prompt_tokens_details`         | string | Detailed token usage information.                        |
| `prompt_logprobs`                     | object | Log probabilities of tokens in the prompt.   |


#### Tasks with region input
    
- Region to segment:

| Tag | Description |
|------|-------------|
| `<REGION_TO_SEGMENTATION>` | Segments salient object in a given region |

- Region to text:
    
| Tag | Description |
|------|-------------|
| `<REGION_TO_CATEGORY>` | Gets object classification for bounding box |
| `<REGION_TO_DESCRIPTION>` | Gets natural language description for contents of bounding box |

This is an example of how a request for a task with region input should look like:

```ts
const modelResponse = await Azion.AI.run("microsoft-florence-2-large", {
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "image_url",
          "image_url": {
            "url": "https://raw.githubusercontent.com/AssemblyAI-Community/florence-2/refs/heads/master/car.png"
          }
        },
        {
          "type": "text",
          "text": "<REGION_TO_DESCRIPTION><loc_39><loc_140><loc_209><loc_366>"
        }
      ]
    }
  ],
  "skip_special_tokens": false,
  "min_tokens": 2,
  "temperature": 0,
  "repetition_penalty": 1.2
})
```

| Property                            | Type | Description                                                                                       |
| ------------------------------------ | ---- | ------------------------------------------------------------------------------------------------- |
| `messages`                           | array | Array of message objects representing the conversation input.                                     |
| `messages[].role`                    | string | Role of the message sender.                                               |
| `messages[].content`                 | array | Array of content blocks sent by the user.                             |
| `messages[].content[].type`          | string | Type of the content block.                                            |
| `messages[].content[].image_url.url` | string | URL of the image to be analyzed.                                                                  |
| `messages[].content[].text`          | string | Text prompt accompanying the image. May contain control tokens. |
| `skip_special_tokens`                | boolean | Whether to remove special tokens from the output.               |
| `min_tokens`                         | number | Minimum number of tokens the model should generate.                                               |
| `temperature`                        | number | Sampling temperature for randomness control.                         |
| `repetition_penalty`                 | number | Penalty to reduce repetition in the output.                    |

Response example:

```json
{
  "id": "chatcmpl-ae746c2c8add44b69677222dc5d148ee",
  "object": "chat.completion",
  "created": 1746823433,
  "model": "microsoft-florence-2-large",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "reasoning_content": null,
        "content": "stop sign<loc_39><loc_140><loc_209><loc_366>",
        "tool_calls": []
      },
      "logprobs": null,
      "finish_reason": "stop",
      "stop_reason": null
    }
  ],
  "usage": {
    "prompt_tokens": 590,
    "total_tokens": 597,
    "completion_tokens": 7,
    "prompt_tokens_details": null
  },
  "prompt_logprobs": null
}
```

| Property                             | Type | Description                                                             |
| ------------------------------------- | ---- | ----------------------------------------------------------------------- |
| `id`                                  | string | Unique ID of the chat completion.                                       |
| `object`                              | string | Type of object returned.                          |
| `created`                             | number | Timestamp when the completion was generated.                     |
| `model`                               | string | Name of the model that generated the response.                          |
| `choices`                             | array | Array of completion choices returned by the model.                      |
| `choices[].index`                     | number | Index of this choice in the list.                                       |
| `choices[].message.role`              | string | Role of the message sender.                     |
| `choices[].message.reasoning_content` | string | Optional field for step-by-step reasoning or justification. |
| `choices[].message.content`           | string | The actual content generated by the model.           |
| `choices[].message.tool_calls`        | array | Array of tool call results.                          |
| `choices[].logprobs`                  | number | Log-probabilities of generated tokens.           |
| `choices[].finish_reason`             | string | Reason why the generation stopped.       |
| `choices[].stop_reason`               | string | Optional alternative stop reason.                           |
| `usage.prompt_tokens`                 | number | Number of tokens used in the prompt.                                    |
| `usage.completion_tokens`             | number | Number of tokens generated in the completion.                           |
| `usage.total_tokens`                  | number | Total number of tokens used.                    |
| `usage.prompt_tokens_details`         | string | Detailed breakdown of prompt tokens.             |
| `prompt_logprobs`                     | number | Log-probabilities for prompt tokens.             |

#### Tasks with natural language input

- Natural language to regions (one to many):

| Tag | Description |
|------|-------------|
| `<PHRASE_GROUNDING>` | Given a caption, provides bounding boxes to visually ground phrases in the caption |

- Natural language to region (one to one):
    
| Tag | Description |
|------|-------------|
| `<OPEN_VOCABULARY_DETECTION>` | Detects bounding box for objects and OCR text |

- Natural language to segment (one to one):

| Tag | Description |
|------|-------------|
| `<REFERRING_EXPRESSION_SEGMENTATION>` | Referring Expression Segmentation - given a natural language descriptor identifies the segmented corresponding region |

This is an example of how a request for a task with natural language input should look like:

```ts
const modelResponse = await Azion.AI.run("microsoft-florence-2-large", {
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "image_url",
          "image_url": {
            "url": "https://raw.githubusercontent.com/AssemblyAI-Community/florence-2/refs/heads/master/car.png"
          }
        },
        {
          "type": "text",
          "text": "<OPEN_VOCABULARY_DETECTION>A green car parked in front of a yellow building."
        }
      ]
    }
  ],
  "skip_special_tokens": false,
  "min_tokens": 2,
  "temperature": 0,
  "repetition_penalty": 1.2
})
```

| Property                            | Type | Description                                                        |
| ------------------------------------ | ---- | ------------------------------------------------------------------ |
| `messages`                           | array | Array of message objects representing the conversation.            |
| `messages[].role`                    | string | Role of the message sender.                  |
| `messages[].content`                 | array | Array of content items within the message. |
| `messages[].content[].type`          | string | Type of contents.                       |
| `messages[].content[].image_url.url` | string | URL of the image to be processed by the model.                     |
| `messages[].content[].text`          | string | Text input to guide the model.           |
| `skip_special_tokens`                | boolean | Boolean indicating whether to skip special tokens in the output.   |
| `min_tokens`                         | number | Minimum number of tokens to generate.                              |
| `temperature`                        | number | Sampling temperature.                        |
| `repetition_penalty`                 | number | Penalty for repeated tokens.                |

Response example:

```json
{
  "id": "chatcmpl-c8a6b72c132143d4aeb685cd7f846f44",
  "object": "chat.completion",
  "created": 1746823810,
  "model": "microsoft-florence-2-large",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "reasoning_content": null,
        "content": " green car parked in front of a yellow building.<loc_53><loc_333><loc_932><loc_774>",
        "tool_calls": []
      },
      "logprobs": null,
      "finish_reason": "stop",
      "stop_reason": null
    }
  ],
  "usage": {
    "prompt_tokens": 596,
    "total_tokens": 611,
    "completion_tokens": 15,
    "prompt_tokens_details": null
  },
  "prompt_logprobs": null
}
```

| Property | Type | Description |
|------------|------|-------------|
| `id` | string | Unique identifier for the response. |
| `object` | string | The type of object returned in the response. |
| `created` | number | Timestamp of when the response was created. |
| `model` | string | The name of the model used for the request. |
| `choices` | array | Array of objects containing the response choices. |
| `choices[].index` | number | The index of the choice in the array. |
| `choices[].message` | object | Object containing the message details. |
| `choices[].message.role` | string | The role of the message sender. |
| `choices[].message.reasoning_content` | null | The reasoning content of the message, if any. |
| `choices[].message.content` | string | The content of the message. |
| `choices[].message.tool_calls` | array | Array of tool calls, if any. |
| `choices[].logprobs` | number | The log probabilities of the choice. |
| `choices[].finish_reason` | string | The reason why the model finished generating text. |
| `choices[].stop_reason` | string | The reason why the model stopped generating text. |
| `usage` | object | Object containing usage metrics for the request. |
| `usage.prompt_tokens` | number | The number of tokens in the input prompt. |
| `usage.total_tokens` | number | The total number of tokens processed. |
| `usage.completion_tokens` | number | The number of tokens in the completion. |
| `usage.prompt_tokens_details` | string | Additional details about the prompt tokens. |
| `prompt_logprobs` | number | The log probabilities of the prompt. |

## JSON schema

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "required": [
    "messages"
  ],
  "properties": {
    "messages": {
      "type": "array",
      "items": {
        "type": "object",
        "required": [
          "role",
          "content"
        ],
        "properties": {
          "role": {
            "type": "string",
            "const": "user"
          },
          "content": {
            "type": "array",
            "maxItems": 2,
            "items": {
              "oneOf": [
                {
                  "type": "object",
                  "required": [
                    "type",
                    "image_url"
                  ],
                  "properties": {
                    "type": {
                      "const": "image_url"
                    },
                    "image_url": {
                      "type": "object",
                      "required": [
                        "url"
                      ],
                      "properties": {
                        "url": {
                          "type": "string",
                          "format": "uri"
                        }
                      }
                    }
                  }
                },
                {
                  "type": "object",
                  "required": [
                    "type",
                    "text"
                  ],
                  "properties": {
                    "type": {
                      "const": "text"
                    },
                    "text": {
                      "type": "string",
                      "enum": [
                        "<OCR>",
                        "<CAPTION>",
                        "<DETAILED_CAPTION>",
                        "<MORE_DETAILED_CAPTION>",
                        "<OCR_WITH_REGION>",
                        "<REGION_PROPOSAL>",
                        "<OD>",
                        "<DENSE_REGION_CAPTION>"
                      ]
                    }
                  }
                },
                {
                  "type": "object",
                  "required": [
                    "type",
                    "text"
                  ],
                  "properties": {
                    "type": {
                      "const": "text"
                    },
                    "text": {
                      "type": "string",
                      "allOf": [
                        {
                          "not": {
                            "pattern": "<OCR>|<CAPTION>|<DETAILED_CAPTION>|<MORE_DETAILED_CAPTION>|<OCR_WITH_REGION>|<REGION_PROPOSAL>|<OD>|<DENSE_REGION_CAPTION>"
                          }
                        },
                        {
                          "pattern": "<REGION_TO_SEGMENTATION>|<REGION_TO_CATEGORY>|<REGION_TO_DESCRIPTION>|<PHRASE_GROUNDING>|<OPEN_VOCABULARY_DETECTION>|<REFERRING_EXPRESSION_SEGMENTATION>"
                        }
                      ]
                    }
                  }
                }
              ]
            }
          }
        }
      }
    }
  }
}
```