---
title: AI Inference
description: >-
  AI Inference enables you to run AI models directly on Azion’s highly distributed infrastructure.
meta_tags: 'ai inference, artificial intelligence, edge computing'
namespace: docs_edge_ai_reference
permalink: /documentation/products/ai/ai-inference/
menu_namespace: AIInferenceMenu

---

import LinkButton from 'azion-webkit/linkbutton';

**AI Inference** enables you to run AI models directly on Azion’s highly distributed infrastructure.

With Azion AI Inference, you can integrate AI capabilities into your applications, leveraging tools like **Functions**, **Applications**, **Vector Search**, and the Azion API to create scalable, secure, and efficient solutions.

Get started by deploying the AI Inference Quick Start:

<LinkButton
    label="Deploy"
    link="https://console.azion.com/create/vue/vue-boilerplate"
    icon="ai ai-azion"  
    icon-pos="left"
  />


---

## Features

### Run AI Inference at the Edge

Execute AI models directly on Azion’s globally distributed infrastructure to reduce latency and enable real-time responses.
Access our catalog of open-source AI models that you can run directly on Azion Runtime. These models are optimized for distributed deployment with minimal resource requirements.

<LinkButton link="/en/documentation/products/ai/ai-inference/models/" label="See Available Models" severity="secondary" />

### Pre-Trained LLMs and VLMs

Use state-of-the-art large language and vision-language models available natively on the Azion platform.

### OpenAI-Compatible API

Connect applications using Azion’s OpenAI-compatible endpoint format.

### Fine-Tune Models with LoRA

AI Inference allows you to fine-tune, train, and specialize models your own data and parameters.. This capability enables you to optimize models for specific tasks, ensuring they are both efficient and accurate for your business needs.

---

### Examples of what you can build with AI Inference

- **Deploy Scalable 24/7 AI Assistants**: Build and deploy AI assistants that serve thousands of users simultaneously with low latency, delivering real-time support, dynamic FAQs, and customer assistance without cloud overload.

- **Build AI Agents**: Build AI agents that automate multi‑step workflows, collapse days of manual effort into minutes, and free teams for higher‑value work—boosting productivity across operations. 

- **Build and Scale AI Applications**: Build scalable, low-latency AI applications that support advanced models, fine-tuning, and seamless integration—enabling real-time processing and interconnected AI solutions that drive innovation and operational efficiency worldwide.

- **Automate Threat Detection and Takedown with AI**: Combine LLMs and vision-language models (VLMs) to monitor digital assets, spot phishing/abuse patterns in text and imagery, and automate threat classification and takedown across distributed environments. 


## Usage

AI Inference can be used in a [Function]

This function receives a POST request to the desired AI model and returns the response.


```javascript
const modelResponse = await Azion.AI.run("Qwen/Qwen3-30B-A3B-Instruct-2507-FP8", {
  "stream": true,
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "Name the european capitals"
    }
  ]
})
return modelResponse
```

This example uses the Qwen3 model. You can change the model and the request parameters according to your preferences. Check the [AI models reference](/en/documentation/products/ai/ai-inference/models/) for more information about the available models and how to use them in your application.


## Integration with SQL Database

Integrate your application with **SQL Database** to enable [vector search](/en/documentation/products/store/sql-database/vector-search/) capabilities, allowing for semantic queries and hybrid search. This integration enhances AI-powered applications by providing precise, contextually relevant results and supporting efficient Retrieval-Augmented Generation (RAG) implementations.

## Limits

300 reqs/minute


Azion Plans:
- Free: 300 reqs/minute
- Pro: 300 reqs/minute
- Business: 300 reqs/minute
- Enterprise: 720 reqs/minute"

---

## Related products

- [Applications](/en/documentation/products/build/applications/): build applications that run directly on Azion's distributed infrastructure, delivering exceptional performance and customization options.
- [Functions](/en/documentation/products/build/applications/functions/): execute code closer to end users, enhancing performance and enabling custom logic for handling requests and responses.
- [SQL Database](/en/documentation/products/store/sql-database/): an edge-native SQL solution designed for serverless applications, providing data storage and querying capabilities at the edge. Also enables [Vector Search](/en/documentation/products/store/sql-database/vector-search/) for performing semantic search and AI-powered recommendations through vector embedding.

---

