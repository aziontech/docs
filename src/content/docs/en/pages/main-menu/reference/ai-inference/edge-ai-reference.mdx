---
title: AI Inference
description: >-
  AI Inference enables you to run AI models directly on Azion’s highly distributed infrastructure.
meta_tags: 'ai inference, artificial intelligence, edge computing'
namespace: docs_edge_ai_reference
permalink: /documentation/products/ai/ai-inference/
menu_namespace: AIInferenceMenu

---

import LinkButton from 'azion-webkit/linkbutton';

**AI Inference** enables you to run AI models directly on Azion’s highly distributed infrastructure.

With Azion AI Inference, you can integrate AI capabilities into your applications, leveraging tools like **Functions**, **Applications**, **Vector Search**, and the Azion API to create scalable, secure, and efficient solutions.


AI Inference gives allows you to:

- **Run AI models closer to your users**, enabling advanced AI architectures to execute directly at the edge for minimal latency and maximum performance.
- **Deploy autonomous AI agents** that analyze data and make decisions.
- **Peform Real-time data processing** with reduced latency and enhanced efficiency.

---

## Features

### Available Models

Access our catalog of open-source AI models that you can run directly on Azion Runtime. These models are optimized for distributed deployment with minimal resource requirements.

<LinkButton link="/en/documentation/products/ai/ai-inference/models/" label="See Available Models" severity="secondary" />

### Model customization

AI Inference allows you to fine-tune, train, and specialize models using **Low-Rank Adaptation (LoRA)**. This capability enables you to optimize models for specific tasks, ensuring they are both efficient and accurate for your business needs.

---

## Usage

AI Inference can be used in a [Function]

This function receives a POST request to the desired AI model and returns the response.

```javascript
const modelResponse = await Azion.AI.run("Qwen/Qwen3-30B-A3B-Instruct-2507-FP8", {
  "stream": true,
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "Name the european capitals"
    }
  ]
})
return modelResponse
```

This example uses the Qwen3 model. You can change the model and the request parameters according to your preferences. Check the [AI models reference](/en/documentation/products/ai/ai-inference/models/) for more information about the available models and how to use them in your application.


## Integration with SQL Database

Integrate your application with **SQL Database** to enable [vector search](/en/documentation/products/store/sql-database/vector-search/) capabilities, allowing for semantic queries and hybrid search. This integration enhances AI-powered applications by providing precise, contextually relevant results and supporting efficient Retrieval-Augmented Generation (RAG) implementations.


---

## Related products

- [Applications](/en/documentation/products/build/applications/): build applications that run directly on Azion's distributed network, delivering exceptional performance and customization options.
- [Functions](/en/documentation/products/build/applications/functions/): execute code closer to end users, enhancing performance and enabling custom logic for handling requests and responses.
- [SQL Database](/en/documentation/products/store/sql-database/): an edge-native SQL solution designed for serverless applications, providing data storage and querying capabilities at the edge.
- [Vector Search](/en/documentation/products/store/sql-database/vector-search/): enable semantic search engines and AI-powered recommendations through vector embeddings at the edge.

---

Explore practical examples of how to implement AI solutions with Azion:

<LinkButton link="/en/documentation/architectures/artificial-intelligence/ai-agent-copilot-assistant/" label="Go to Copilot Assistant architecture" severity="secondary" />
<LinkButton link="/en/documentation/products/guides/langgraph-ai-agent-boilerplate/" label="Go to the LangGraph AI Agent template guide" severity="secondary" />