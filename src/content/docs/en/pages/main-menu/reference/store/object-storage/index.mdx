---
title: Object Storage
description: >-
  Azion Object Storage is a scalable and secure storage service designed to
  integrate object storage with the Azion Web Platform using the S3
  standard.
meta_tags: 'object storage, storage, cloud, s3, bucket, file storage, object storage'
namespace: documentation_products_store_object_storage
permalink: /documentation/products/store/object-storage/
---

import Tag from 'primevue/tag';
import Button from 'azion-webkit/components/Button';

<Tag severity="info" client:only="vue">
  Preview
</Tag>

Azion **Object Storage** is a scalable and secure storage service designed to integrate object storage with the Azion Web Platform using the S3 standard for object operations. 

Object Storage allows you to create buckets, which can be used as origins for applications or as directories for real-time object upload. Alongside bucket creation, you possess complete control over storage allocation, bucket and object access management, as well as the ability to upload, change, and delete objects.

:::note
This product is available through the [Azion API](https://api.azion.com) and [Azion Runtime](/en/documentation/runtime/overview/).
:::

## Implementation

| Scope | Resource |
| --- | --- |
| Manage a bucket | [How to create and modify an Object Storage bucket](/en/documentation/products/guides/create-and-modify-bucket/) |
| Upload and download objects | [How to upload and download objects from an Object Storage bucket](/en/documentation/products/guides/upload-and-download-objects-from-bucket/) |
| Use bucket as origin | [How to use an Object Storage bucket as the origin of a static application](/en/documentation/products/store/storage/use-bucket-as-origin/) |
| Set up the S3 protocol | [How to access an Object Storage bucket using the S3 protocol](/en/documentation/products/store/storage/s3-protocol-for-edge-storage/) |
| Runtime API | [Object Storage API](/en/documentation/runtime/api-reference/storage/) |

---

## Buckets

Buckets are the system used to organize stored objects. Similar to folders, buckets are the top-level containers to store objects. Buckets can be created using the [Azion API](https://api.azion.com/v4#/operations/PostEdgeStorageBuckets).

All buckets created with Azion **Object Storage** are stored in the *us-east* cloud server.

Bucket names are exclusive between all Azion accounts. Names must range between 6 and 63 characters and must not start with `azion`. Alphanumeric characters and hyphen (`-`) are accepted.

Best practices for naming buckets include specifying what types of objects are stored and the type of permissions for the objects. For example, a bucket for an application `Banking App` in read-only mode could be named `banking-app-ro`.

---

## Objects

Objects, or files, can be uploaded, modified, downloaded, and removed from buckets using the [Azion API](https://api.azion.com), [Azion Runtime](/en/documentation/runtime/overview/), and the [S3 protocol](#s3-protocol-compatibility).

### Object key

An object key is a string of characters that composes a unique identifier for objects stored in **Object Storage** buckets. Through the available tools, users can retrieve a file stored in a bucket by using its object key.

The object key isn't required to match the original file path or name from the local storage from which it was retrieved, nor contain the original file extension. However, when uploading a local file to a bucket, it's recommended to attribute the object key after the file to match local storage conventions. For example, for the local file `folder/file.png`, the object key should be the same.

The object key cannot be changed. Uploading a different object or modifying object contents using an existing key replaces the object. Once an object is replaced, earlier versions can't be retrieved.

### Prefix

A prefix is a combination of paths that simulate a folder hierarchy. Since buckets can't be organized into folders, you can use the forward slash (`/`) when creating keys to categorize objects in your bucket into a prefix.

For instance, the list of keys below represents the simulated hierarchy of an application stored in a bucket with prefixes:

```bash
README.md
src/index.js
src/index.html
src/assets/styles.css
src/assets/images/image.png
```

The object `README.md` is located at the root of the bucket. The `src` prefix corresponds to a folder and contains the objects `index.js` and `index.html`. Additionally, the `src/assets` prefix contains a `styles.css` object and the `src/assets/images` prefix, in turn, contains the `image.png` object.

When creating an Object Storage origin, you can set a prefix to serve to the edge. For instance, using the example above, you can create an origin that only serves the `image.png` object by setting the prefix to `src/assets/images`.

### Origin

With **Object Storage**, you can use buckets as an [origin](/en/documentation/products/secure/connector/#connector-type) in Azion **Connectors** to retrieve the content of an application.

You can determine if the content is retrieved from the root of the bucket or from a prefix within the bucket.

---

## Operations

An operation refers to any data exchange between a client and **Object Storage**. All actions related to buckets and objects, such as create, delete, list, and update, are considered an operation. Each time one of these methods is used, an operation is logged through the API or the S3 protocol. 

The current release of the **Object Storage** offers operations listed below, depending on whether the [Azion API](#azion-api-operations) or the [S3 protocol](#s3-operations) is used.

### Azion API operations

:::tip
All the API operations are available in the [Azion API](https://api.azion.com/v4#/operations/GetEdgeStorageBuckets) documentation.
:::

| Class | Operation name | HTTP method |
|-------|-----------------------|-------------|
| A | [ListObjects](#listobjects) | `GET` |
| A | [CreateBucket](#createbucket) | `POST` |
| A | [ListBuckets](#listbuckets) | `GET` | 
| A | [UpdateBucket](#updatebucket) | `PATCH` |
| B | [GetObject](#getobject) | `GET` |
| C | [PostObject](#postobject) | `POST` | 
| C | [PutObject](#putobject) | `PUT` |
| C | [DeleteObject](#deleteobject) | `DELETE` |
| C | [DeleteBucket](#deletebucket) | `DELETE` |

:::note
Class C operations are free. If an operation falls out of the scope of class C operations, it'll be billed to your account according to the established costs. For more operation and class charges information, see the [Pricing](/en/documentation/products/pricing/) page.
:::

#### ListObjects

Retrieves a list of objects loaded into a bucket.

This operation returns details of all objects in the bucket, including the size in bytes and the timestamp of the last modification.

#### CreateBucket
Creates a new bucket for an account.

#### ListBuckets

Retrieves a list of buckets associated with an account.

#### UpdateBucket

Modifies bucket information.

Use this operation to change the access permissions to the objects in the bucket. Buckets cannot be renamed with this operation.

#### GetObject

Retrieves an object from a bucket.

:::tip
For API cURL requests, add the `-O -J` options to the command to download the object to your device using this operation.
:::

#### PostObject

Uploads an object to a bucket. Objects are limited to a maximum size of 20 MB.

:::tip
For API cURL requests, use the `--data-binary` option in the command to send raw binary data, such as images or complex files without undergoing URL encoding.
:::

For the **Azion API**, you can specify the MIME type of the object being sent in the body using the `Content-Type` header. For example, objects with the `.txt` extension should contain the `Content-Type: text/plain` header. If the MIME type isn't specified, **Object Storage** will attempt to interpret the file type based on the file extension. Alternatively, use the `application/octet-stream` MIME type to indicate that the data is a binary stream and the server should handle it as raw binary data.

Sending a new object with an object key already in the bucket will replace the previous object.

#### PutObject

Uploads an object to a bucket.

Sending a new object with an object key that already existed in the bucket will replace the previous object.

#### DeleteObject

Removes an object from a bucket.

When you delete an object being served on the edge, it'll immediately stop being served and will no longer be listed in the bucket.

:::caution[warning]
For S3 credential access, this operation is allowed through the capability `deleteFiles`, which requires the `writeFiles` capability to be enabled.
:::

#### DeleteBucket

Removes a bucket from an account.

Buckets that contain objects cannot be deleted. After removing the final object from a bucket, there is a 24-hour period before the bucket can be deleted.

### S3 operations

:::note
The operations listed in this documentation are the ones currently supported by Azion. However, the full range of possible S3 operations is broader.
:::

| Class | Operation name |
| --- | --- |
| A | [ListBuckets](#listbuckets) | 
| A | [HeadBucket](#headbucket) |
| A | [ListMultipartUploads](#listmultipartuploads) |
| A | [ListObjects](#listobjects) | 
| A | [ListObjectsV2](#listobjectsv2) |
| B | [CopyObject](#copyobject) |
| B | [GetObject](#getobject) |
| B | [HeadObject](#headobject) |
| C | [DeleteObject](#deletebbject) |
| C | [DeleteObjects](#deleteobjects)  |
| C | [AbortMultipartUpload](#abortmultipartupload) |
| C | [CompleteMultipartUpload](#completemultipartupload) |
| C | [CopyObject](#copyobject) |
| C | [CreateMultipartUpload](#createmultipartupload) |
| C | [ListParts](#listparts) | 
| C | [PutObject](#putobject) |
| C | [UploadPart](#uploadpart) |

:::note
Class C operations are free. If an operation falls out of the scope of class C operations, it'll be billed to your account according to the established costs. For more operation and class charges information, see the [Pricing](/en/documentation/products/pricing/) page.
:::

> If `listBuckets` is enabled, when attempting to retrieve files that aren't in the bucket using an S3 credential, the proper `404 Not Found` status response returns instead of a `403 Forbidden` status. Find out more about S3 capabilities in [S3 protocol compatibility](#s3-protocol-compatibility).

Read more on [How to access Object Storage using the S3 protocol](/en/documentation/products/store/storage/s3-protocol-for-edge-storage/).

#### ListBuckets

Retrieves a list of buckets associated with an account.

S3cmd command: `s3cmd ls`

#### HeadBucket

Checks bucket existence and permissions, returning **200 OK** if it does exist or **404 Not Found** if it doesn't.

S3cmd command: `s3cmd info s3://BUCKET`

#### ListMultipartUploads

Lists in progress multipart uploads in a bucket. It refers to a multipart upload that the S3 has initiated a `Create Multipart Upload` request but hasn't yet been completed or aborted.

S3cmd command: `s3cmd multipart s3://BUCKET`

#### ListObjects

Returns a list of up to 1,000 objects in the bucket, sorted alphabetically by key. You can use the query parameters to filter the search.

S3cmd command: `s3cmd ls s3://BUCKET`

For more than 1,000 results, it's recommended to use [ListObjectsV2](#listobjectsv2)

#### ListObjectsV2

Returns a list of up to 1,000 objects in the bucket, sorted alphabetically by key. You can use the query parameters to filter the search.

This limit is a default setup. However, if the search results in more than the maximum result set size, then the first set is returned in the initial response, the `<IsTruncated>` response element contains the value true and the `<NextContinuationToken>` element contains a token to retrieve the next set of results.

Use this token as the `continuation-token` query parameter in another request to retrieve the next set of results.

S3cmd command: `s3cmd ls s3://BUCKET`

#### CopyObject

Creates a copy of an object that is already stored.

S3cmd command: `s3cmd cp s3://BUCKET1/OBJECT1 s3://BUCKET2/OBJECT2`

#### GetObject

Retrieves an object from a bucket.

S3cmd command: `s3cmd get s3://BUCKET/OBJECT LOCAL_FILE`

#### HeadObject

Retrieves metadata from an object without returning the object itself.

S3cmd command: `s3cmd info s3://BUCKET/OBJECT`

#### DeleteObject

Removes an object from a bucket entirely. 


S3cmd command: `s3cmd del s3://BUCKET/OBJECT`

#### DeleteObjects

Deletes multiple objects from a bucket in a single request. In the XML body, provide the object keys and, optionally, version IDs if you want to delete a specific object version.

S3cmd command: `s3cmd del s3://BUCKET/PREFIX --recursive`

#### AbortMultipartUpload
Aborts a multipart upload. 

S3cmd command: `s3cmd abortmp s3://BUCKET/OBJECT Id`

#### CompleteMultipartUpload

Completes a `multipart upload` by assembling previously uploaded parts.

This operation is part of the S3 multipart upload process, being handled automatically as part of the put process when uploading large files once all parts are successfully uploaded.

#### CreateMultipartUpload

Initiates a `multipart upload` and returns a `uploadId`. This `uploadId` is used to associate all of the parts in the specific multipart upload.

This operation is part of the S3 multipart upload process, being handled automatically as part of the put process when uploading large files when you use the put command with a large file. There's no need to explicitly call this operation.

#### ListParts

Lists parts that have been uploaded for a given multipart upload.

This limit is a default setup. However, if the search results number is more than the maximum result set size, then the first set is returned in the initial response, the response element contains the value true and the element contains a token to retrieve the next set of results. Use this token as the part-number-marker query parameter in another request to retrieve the next set of results.

S3cmd command: `s3cmd listmp s3://BUCKET/OBJECT Id`

#### PutObject

Uploads an object to a bucket.

S3cmd command: `s3cmd put FILE s3://BUCKET/OBJECT`

#### UploadPart

Uploads a part of the file in a multipart upload. s3cmd automatically splits large files into parts and uploads them. You don't need to manage the parts or call this operation directly manually.

---

## Authentication and permissions

You can manage the following permissions for Object Storage:

- **Azion Teams Permissions**: operations involving buckets, such as uploading, listing, and deleting objects, always require authentication through the Azion account. Go to [Teams Permissions](/en/documentation/products/accounts/teams-permissions/) to know more about the available Object Storage account permissions.
- **Bucket permissions**: manage *access from the edge and other users to buckets and objects within buckets*; related to the `edge_access` attribute. 
- **S3 credentials**: manage *access for Azion account users through capabilities and assign operation permissions* exclusive to S3 protocol access.

:::caution[Warning]
The bucket's access permissions, related to the `edge_access` attribute, apply only to the way the edge can access the bucket. This means that even if a bucket is set to `read_only` or `restricted`, a `PutObject operation can still be performed through the S3 protocol or REST APIs if the credentials used include the `writeFiles` capability. 
:::

### Bucket permissions

In addition to the required authentication and necessary permissions, some API operations can be restricted by bucket permissions. The permissions available are:

- **Read-only**: objects in the bucket can be read but not modified by the Azion Web Platform.
- **Read-write**: objects in the bucket can be modified by the Azion Web Platform.
- **Restricted**: objects in the bucket can be modified and read but can't be accessed by the Azion Web Platform. Restricted buckets can't be modified using Azion Runtime and can't be used as an origin for applications.

These permissions are related to the way the edge can access the bucket. For instance:

- If the bucket is set to `read-only`, the Azion Web Platform can retrieve objects from the bucket but cannot upload or modify them. However, authorized users can continue writing to Object Storage through the API or the S3 protocol.
- If the bucket is set to `read-write`, the Azion Web Platform and other users can both retrieve and modify objects within the bucket.
- If the bucket is set to `restricted`, the Azion Web Platform cannot access the bucket's content. In this case, only authorized users can continue writing to Object Storage through the API or the S3 protocol. 

For example, when an external user attempts to send a `POST` or `PUT `request to an application using a bucket configured with read-only or restricted permissions as its origin, the edge will deny access and return an error message.

:::caution[Warning]
When a bucket is configured with **read-write permissions**, any user can access and modify its content (for example, by sending a PUT request to overwrite files). If the bucket is used as an origin for an application, this configuration may expose the content to unauthorized modifications. However, suppose the bucket is accessed through an function. In that case, the risk is managed by the logic implemented in the function code, allowing developers to enforce stricter controls over who can access or modify the data. To mitigate risks, always verify the required permissions for each use case and implement proper access controls.
:::

### S3 credentials

**Object Storage** offers compatibility with the S3 protocol through credentials. 

Credentials can be created for any bucket that you own or for your account as a whole to manage all your buckets. With them, you can control permissions for operations associated with that credential. The permissions for the credential are exclusive to access thorugh the S3 protocol.

To [create a S3 credential](/en/documentation/products/store/storage/s3-protocol-for-edge-storage/), you must use an Azion personal token and run a `POST` request via API. However, after the credential is created, it works independently from your Azion token. This way, even if the token expires, the credential remains valid.

Once a credential is created, an *access key* and a *secret key* are generated, which can be used to set up access to the bucket through the S3 protocol. For security reasons, the *secret key* won't be available after the credential is created. Existing credentials can't be modified in any way.

Once a user's access is verified, they're allowed to make operations depending on the capabilities and permissions set for the credential. 

#### Capabilities

You can assign the following capabilities to S3 credentials:

- `listFiles`: equivalent to [ListObjects](#listobjects), returns a list of objects within the bucket.
- `readFiles`: equivalent to [GetObject](#getobject), returns an object from the bucket through the object key.
- `writeFiles`: equivalent to [PutObject](#putobject), allows modifying files in the bucket through the object key.
- `deleteFiles`: equivalent to [DeleteObject](#deleteobject), allows object deletion through the object key.
- `listAllBucketNames`: equivalent to [ListBuckets](#listbuckets), allows you to list all buckets associated with the account.
- `listBuckets`: if enabled, returns the proper `404 Not Found` response when attempting to retrieve files that aren't in the bucket using the credential.

---

## S3 protocol compatibility

After an [S3 credential](#s3-credentials) is created for a bucket, you can use the S3 protocol (`s3://`) to execute operations according to the [list of capabilities](#capabilities). 

The S3 protocol allows you to access buckets and objects using an Object Storage URL. This configuration facilitates file operations through command line interface (CLI) tools, such as [s3cmd](https://s3cmd.org), database services, or functions.

You can use the access and secret keys provided by the [S3 credentials API](https://api.azion.com) to set up a connection using the S3 protocol.

<Button href="/en/documentation/products/store/storage/s3-protocol-for-edge-storage/" label="learn How to access Object Storage using the S3 protocol" type="primary" theme="light" />

To do so, you'll need the following information:

| Data | Description |
| --- | --- |
| Access key | The credential's access key generated upon creating the S3 credential with the Azion API |
| Secret key | The credential's secret key generated upon creating the S3 credential with the Azion API. This information is confidential and will only be available at the moment of creation |
| Region | The assigned server's region, which is `us-east-005` |
| S3 endpoint | The default S3 address for all operations, which is `s3.us-east-005.azionstorage.net` |
| DNS-style template | The host name template to access the bucket and objects. Can be `bucket+hostname:port/object-key` or `hostname:port/bucket`. <br /><br />For example, for a `file.txt` object in the `my-bucket` bucket, the host names could be: <br /><li><ul>`my-bucket.s3.us-east-005.azionstorage.net/file.txt`</ul><ul>`s3.us-east-005.azionstorage.net/my-bucket/file.txt`</ul></li> |

---

## Limits

:::tip
**Increase limits** <br></br>
You can request to increase the limits based on your plan. Contact the [technical support team](/en/documentation/services/support/) to request it.
:::

These are the **default limits**:

| Scope | Limit | 
| --- | --- | 
| Buckets | 100 per account |
| Region | us-east |
| S3 credential access keys | 100,000 per account |


Learn how Azion Object Storage scales unstructured data effortlessly. Watch the video below:
<div class="cms-embed">
  <iframe width="600" height="400" src="https://www.youtube.com/embed/vDxJV4T-Qc8?si=YdRN4gVZwU0A3str" loading="lazy" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</div>
