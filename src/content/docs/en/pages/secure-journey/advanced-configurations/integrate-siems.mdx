---
title: How to integrate WAF with SIEMs
description: Use Data Streaming to integrate your WAF with SIEM platforms.
meta_tags: edge, secure, waf, siem, logs, data streaming
namespace: docs_secure_automate_integrate_siems
menu_namespace: secureMenu
permalink: /documentation/products/guides/secure/automate/integrate-siems/
---

import Button from '~/components/Button.astro'

Your logs from [Web Application Firewall (WAF)](/en/documentation/products/edge-firewall/web-application-firewall/) can be integrated with SIEM platforms through **Data Streaming** to monitor your edge applications behaviors, performance, and security.

<Button href="/en/documentation/products/data-streaming/" text="go to data streaming reference" variant="secondary"></Button>

---

## Via RTM

1. [Access RTM](/en/documentation/products/guides/how-to-access-rtm/) > **Data Streaming**.
2. Click **Add Streaming**.
3. Choose a unique and easy-to-remember name.
4. On the **Data Source** dropdown menu, select **Edge Applications**.
5. On the **Template** dropdown menu, select **Edge Applications + WAF Event Collector**.
6. On **Options**, select between **Filter Domains** or **All Domains**. Find more information on each option on [How to associate domains on Data Streaming](/en/documentation/products/guides/data-streaming-associate-domains/).
7. On the **Destination** section, select an **Endpoint Type** on the dropdown menu: **Standard HTTP/HTTPS POST**, **Apache Kafka**, **Simples Storage Service (S3)**, **Google BigQuery**, **Elasticsearch**, **Splunk**, **AWS Kinesis Data Firehose**, **Datadog**, **IBM QRadar**, **Azure Monitor**, or **Azure Blob Storage**.
    - You'll see different fields depending on the endpoint type you choose. Find more information on each of them on the specific guide for the endpoint on the [Observe guides section](/en/documentation/products/guides/#observe).
8. Click the **Save** button.

---

## Via API

1. Run the following `POST` request, replacing `[TOKEN VALUE]` with your [personal token](/en/documentation/products/guides/personal-tokens/): 

```bash
curl --location 'https://api.azionapi.net/data_streaming/streamings' \
--header 'Accept: application/json; version=3' \
--header 'Authorization: Token [TOKEN VALUE]' \
--header 'Content-Type: application/json' \
--data '{
    "name": "Kafka Connector",
    "template_id": 184,
    "domain_ids": [1656613172],
    "data_source": "http",
    "endpoint": {
        "endpoint_type": "kafka",
        "kafka_topic": "mykafka.dts.topic",
        "bootstrap_servers": "infra.my.net:9094,infra.my.net:9094"
    },
    "all_domains": false
}'
```

2. You'll receive a response similar to this:

```json
{
  "results": {
    "id": 1594,
    "name": "Kafka Connector",
    "template_id": 184,
    "data_source": "http",
    "active": true,
    "endpoint": {
      "endpoint_type": "kafka",
      "use_tls": false,
      "kafka_topic": "mykafka.dts.topic",
      "bootstrap_servers": "infra.my.net:9094,infra.my.net:9094"
    },
    "all_domains": false
  },
  "schema_version": 3
}
```

:::tip
This example uses the **Apache Kafka endpoint**, but you can change the `endpoint_type` to any of the available endpoints and add the specific mandatory fields. See the available endpoints and their fields in the [Data Streaming Endpoints](https://api.azion.com/#b9aac0c7-173a-462b-87f6-a18d6558d3e5) table.
:::

Wait a few minutes for the changes to propagate and your data streaming will be created.

:::tip
Check the [Azion API documentation](https://api.azion.com/) and the [OpenAPI specification](https://github.com/aziontech/azionapi-openapi/) to know more about what the Azion API can offer.
:::
