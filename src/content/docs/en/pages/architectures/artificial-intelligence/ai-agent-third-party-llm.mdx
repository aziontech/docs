---
title: AI Agents Configuration for Third Party LLM Providers
description: >-
  Learn how to integrate an AI Agent, based on the LangGraph AI Agent Boilerplate, with Third Party LLM, such as OpenAI and Anthropic, to create a personalized AI chatbot.
meta_tags: ai, chatbot, large language model, llm, openai, gpt, langgraph, langchain, anthropic, claude
namespace: docs_arch_ai_agent_third_party_llm
permalink: /documentation/architectures/artificial-intelligence/ai-agent-third-party-llm/
---
import LinkButton from 'azion-webkit/linkbutton';
import Tag from 'primevue/tag';

This architecture describes an AI agent at the edge that consumes Third Party LLM via API, stores state in SQL Database, and prioritizes minimal latency.


## Azion AI Agent architecture diagram with Third party integration

![AI Agent with Third-Party LLM Architecture Diagram](/assets/docs/images/uploads/diagram-3rd-party-agent.png)

<p class="text-center text-sm"><b>Figure 1.</b> AI agent architecture diagram with a Third Party LLM. </p>

### Data flow

1. A request arrives at the Azion Web Platform.
2. The Frontend Application serves the user interface (with static files from Object Storage, e.g., CSS, HTML, etc.).
3. The frontend application sends an API request to the backend function.
4. The backend function reads/writes chat state to SQL for persistence and context, then executes the LangGraph agent and streams the response to the client.
5. The agent calls Third Party LLMs, processes user input using database data to formulate a response, and sends it back to the user through the same path.
6. The Frontend Application displays the response to the user.

### Components

- **[Applications](https://www.azion.com/en/documentation/products/build/applications/)**: an application on Azion that hosts the AI agent.
- **[Functions](https://www.azion.com/en/documentation/products/build/applications/functions/)**: a serverless function that contains the AI agent logic.
- **[Object Storage](https://www.azion.com/en/documentation/products/store/object-storage/)**: Azion's object storage service to store the data that the agent uses to answer questions.
- **[SQL Database](https://www.azion.com/en/documentation/products/store/sql-database/)**: Azion's database to store chat state and documents.
- **Third Party LLM**: LLM service (e.g., OpenAI or Anthropic).

---

## Implementation

Use the [LangGraph AI Agent Boilerplate](/en/documentation/products/guides/langgraph-ai-agent-boilerplate/) to quickly set up a LangGraph backend and SQL base that ensures reliable persistence of state and documents.

You can obtain and configure your template through the Azion Console. To easily deploy it at the edge, click the button below.

<LinkButton
    label="Deploy"
    link="https://console.azion.com/create/azion/langgraph-agent-boilerplate"
    icon="ai ai-azion"
    icon-pos="left"
  />

1. Access the Azion Console.
2. Click `+ Create` to open the templates page.
3. Select `LangGraph AI Agent Boilerplate`.
4. Connect your GitHub, define the agent name, and provide the necessary keys (e.g., OpenAI API Key).
5. Choose the authentication method (No Auth, Basic, Clerk) and finish with `Deploy`.
6. Wait for propagation and access the domain `your-url.map.azionedge.net` (or your custom domain).

### File upload on the platform

Once the above process has been implemented, your database is already configured and ready for use. 
Let's load the documents that the agent will use as context. 

- Clone locally the template backend implemented in your Github account.
- Create a .env file with the necessary configurations for the template backend.

```bash
ZION_TOKEN='your_azion_personal_token'
OPENAI_API_KEY='API_KEY'
OPENAI_MODEL= 'model'
EMBEDDING_MODEL= 'embedding-model'

# SQL Database and table names. Usually, table names are not changed,
# and the database name is the name of your agent + database: agent_name_database
MESSAGE_STORE_DB_NAME='youragent-database'
MESSAGE_STORE_TABLE_NAME='messages'
VECTOR_STORE_DB_NAME='youragent-database'
VECTOR_STORE_TABLE_NAME='vectors'

# Optional: Langsmith to trace the requests and responses
LANGSMITH_API_KEY=
LANGCHAIN_PROJECT=
LANGCHAIN_TRACING_V2=false

# Optional: Clerk authentication with your frontend
CLERK_PUBLISHABLE_KEY=
CLERK_SECRET_KEY=
```
- Insert the desired files in the `files` subfolder of the `migrations` directory. If the folder doesn't exist, create it. Currently, PDF, MD, JSON, and TXT files are supported.
- In the terminal, go to the `migrations` folder and run the `yarn setup` command to create the tables in the database.
- Then, run the `yarn upload-docs` command to upload the files to Object Storage.

:::note
This template uses [Application Accelerator](/en/documentation/products/build/applications/application-accelerator/), [Functions](/en/documentation/products/build/applications/functions/), and [SQL Database](/en/documentation/products/store/sql-database/), and may generate usage-related costs. Check the [pricing page](https://www.azion.com/en/pricing/) for more information.
:::
---

## Reference Guide

- [Reference Guide - How to integrate an AI agent with Third Party LLM](/en/documentation/products/guides/langgraph-ai-agent-boilerplate/)

## Related Documentation

- [Object Storage](/en/documentation/products/store/object-storage/)
- [SQL Database](/en/documentation/products/store/sql-database/)
- [How to configure a custom domain](/en/documentation/products/guides/configure-domain/)
- [Azion GitHub App](/en/documentation/products/guides/azion-github-app/)
- [Vector Search with SQL Database](/en/documentation/products/guides/sql-database-vector-search/)
