---
title: Implemenete AI Inference em suas aplicações
description: Referência de arquitetura para executar inferência de modelos de IA no edge, com baixa latência e alta eficiência, usando Applications, Functions e SQL Database com Vector Search da Azion.
meta_tags: ai, inferência, edge, latency, inteligência artificial, vector search, applications, functions
namespace: docs_arch_ai_inference
permalink: /documentacao/arquiteturas/inteligencia-artificial/ai-inference-architecture/
---
import LinkButton from 'azion-webkit/linkbutton';

O AI Inference no edge executa modelos junto às fontes de dados para reduzir latência, preservar privacidade e otimizar uso de recursos.

## Diagrama da arquitetura de AI Inference

A figura a seguir ilustra uma arquitetura de referência para inferência de IA no edge.

![Arquitetura de AI Inference no Edge](/assets/docs/images/uploads/diagram-ai-inference.png)

### Fluxo de dados

1. O usuário envia uma requisição para o domínio da aplicação.
2. O edge node mais próximo recebe a requisição e encaminha para a **aplicação** que aplica políticas e chama a função.
3. A **função** orquestra a inferência: pré-processa entrada, e chama o modelo via `Azion.AI.run` através de API do Azion Cells.
4. O agente processa a entrada do usuário e a envia de volta ao usuário pelo mesmo caminho.


## Componentes

- **Applications**: camada de aplicação na rede distribuída da Azion.
- **Functions**: executa a lógica de inferência, orquestra RAG e integra serviços externos.
- **AI Inference (Edge Runtime)**: executa modelos de IA diretamente no edge.
- **Global Infrastructure**: rede global distribuída da Azion.
- **Orchestrator**: gerencia requisições através do Edge Node para execução de Cells.

**Obs:** O Azion Cells é uma funcionalidade avançada que faz parte do Orchestrator. Para ativá-lo, é necessário entrar em contato com a equipe de vendas da Azion para verificar os requisitos e habilitar o produto na sua conta.

---
## Implementação

Use o template [AI Inference Starter Kit](/pt-br/documentacao/produtos/guias/ai-inference-starter-kit/) para acelerar sua jornada e ter uma aplicação pronta com APIs compatíveis com OpenAI, rodando no edge.


1. Acesse o console da Azion.
2. Clique no botão `+ Create` para abrir a página de templates.
3. Selecione o template `AI Inference Starter Kit`.
4. Na página do template, digite um nome para sua aplicação e clique em `Deploy`.
5. Aguarde a aplicação estar disponível no domínio `your-url.map.azionedge.net` (ou seu domínio personalizado).
6. Interaja com o modelo via endpoint OpenAI-compatível da sua aplicação.

Exemplo de interação com o modelo `nanonets/Nanonets-OCR-s` que é um modelo OCR orientado a instruções que extrai texto de imagens/documentos:

```bash
curl -X POST https://YOUR-DOMAIN.map.azionedge.net/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nanonets/Nanonets-OCR-s",
    "max_tokens": 500,
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "image_url",
            "image_url": { "url": "https://www.any-url-example.com/example.jpg" }
          },
          {
            "type": "text",
            "text": "Extraia o texto do documento acima em Markdown estruturado. Tabelas em HTML. Equações em LaTeX. Use <watermark>...</watermark> para marcas d'água e <page_number>...</page_number> para paginação."
          }
        ]
      }
    ]
  }'
```

Observações:
- Acesse a página dos [Modelos do AI Inference da Azion](/pt-br/documentacao/produtos/ai/ai-inference/modelos/) para ver mais modelos disponíveis.
- Se você adicionou autenticação na sua API, inclua o header apropriado (por exemplo, `-H "Authorization: Bearer <TOKEN>"`).


Exemplo de uma função que executa Azion.AI.run dentro de uma Function:

```ts
async function handleRequest(request) {
  const url = new URL(request.url);
  if (url.pathname !== "/ocr") return new Response("Not found", { status: 404 });

  const modelResponse = await Azion.AI.run("nanonets/Nanonets-OCR-s", {
    stream: false,
    max_tokens: 500,
    messages: [
      {
        role: "user",
        content: [
          {
            type: "image_url",
            image_url: { url: "https://www.any-url-example.com/example.jpg" }
          },
          {
            type: "text",
            text:
              "Extraia o texto do documento acima em Markdown estruturado. " +
              "Tabelas em HTML. Equações em LaTeX. Use <watermark>...</watermark> " +
              "para marcas d'água e <page_number>...</page_number> para paginação."
          }
        ]
      }
    ]
  });

  return new Response(JSON.stringify(modelResponse), {
    headers: { "Content-Type": "application/json" }
  });
}

addEventListener("fetch", (event) => {
  event.respondWith(handleRequest(event.request));
});
```


## Observabilidade

- **[Real-Time Metrics](https://www.azion.com/pt-br/documentacao/produtos/observe/real-time-metrics/)**: analise os eventos e obtenha insights sobre o desempenho de suas aplicações.
- **[Real-Time Events](https://www.azion.com/pt-br/documentacao/produtos/observe/real-time-events/)**: rastreie requisições, dados brutos, logs e pesquisas complexas.


---

## Documentação relacionada

- [AI Inference](/pt-br/documentacao/produtos/ai/ai-inference/)
- [Guia do AI Inference Starter Kit](/pt-br/documentacao/produtos/guias/ai-inference-starter-kit/)
- [Modelos disponíveis](/pt-br/documentacao/produtos/ai/ai-inference/modelos/)

