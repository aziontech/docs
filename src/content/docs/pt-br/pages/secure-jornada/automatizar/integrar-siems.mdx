---
title: Como integrar WAF com SIEMs
description: Use o Data Stream para integrar o seu Web Application Firewall (WAF) com plataformas de SIEM.
meta_tags: edge, secure, waf, siem, logs, Data Stream
namespace: docs_secure_automate_integrate_siems
menu_namespace: secureMenu
permalink: /documentacao/produtos/secure/automarizar/integrar-siems/
---

import Button from '~/components/Button.astro'
import ContributorList from '~/components/ContributorList.astro'

Seus registros do [Web Application Firewall (WAF)](/pt-br/documentacao/produtos/edge-firewall/web-application-firewall/) podem ser integrados a plataformas Security Information and Event Management (SIEM) por meio do **Data Stream** para monitorar comportamentos, desempenho e segurança de suas edge applications.

<Button href="/pt-br/documentacao/produtos/observe/data-stream/" text="saiba mais sobre Data Stream" variant="secondary"></Button>

---

## Via Azion Console

1. [Acesse o Azion Console](/pt-br/documentacao/produtos/guias/como-acessar-o-rtm/) > **Data Stream**.
2. Clique em **Add Streaming**.
3. Escolha um nome único e fácil de lembrar.
4. No menu suspenso **Data Source**, selecione **Edge Applications**.
5. No menu suspenso **Template**, selecione **Edge Applications + WAF Event Collector**.
6. Em **Options**, escolha entre **Filter Domains** ou **All Domains**. Encontre mais informações sobre cada opção em [Como associar domínios no Data Stream](/pt-br/documentacao/produtos/guias/data-stream-associar-dominios/).
7. Na seção **Destination**, selecione um **Endpoint Type** no menu suspenso: **Standard HTTP/HTTPS POST**, **Apache Kafka**, **Simple Storage Service (S3)**, **Google BigQuery**, **Elasticsearch**, **Splunk**, **AWS Kinesis Data Firehose**, **Datadog**, **IBM QRadar**, **Azure Monitor** ou **Azure Blob Storage**.
    - Você verá campos diferentes dependendo do tipo de endpoint escolhido. Encontre mais informações sobre cada um deles no guia específico para o endpoint na seção de [Observe](/pt-br/documentacao/produtos/guias/#observe) na página de guias.
8. Clique no botão **Save**.

---

## Via API

1. Execute a seguinte requisição `POST`, substituindo `[TOKEN VALUE]` pelo seu [personal token](/pt-br/documentacao/produtos/guias/personal-tokens/):

```bash
curl --location 'https://api.azionapi.net/data_streaming/streamings' \
--header 'Accept: application/json; version=3' \
--header 'Authorization: Token [TOKEN VALUE]' \
--header 'Content-Type: application/json' \
--data '{
    "name": "Conector Kafka",
    "template_id": 184,
    "domain_ids": [1656613172],
    "data_source": "http",
    "endpoint": {
        "endpoint_type": "kafka",
        "kafka_topic": "mykafka.dts.topic",
        "bootstrap_servers": "infra.my.net:9094,infra.my.net:9094"
    },
    "all_domains": false
}'
```

2. Você receberá uma resposta semelhante a esta:

```json
{
  "results": {
    "id": 1594,
    "name": "Conector Kafka",
    "template_id": 184,
    "data_source": "http",
    "active": true,
    "endpoint": {
      "endpoint_type": "kafka",
      "use_tls": false,
      "kafka_topic": "mykafka.dts.topic",
      "bootstrap_servers": "infra.my.net:9094,infra.my.net:9094"
    },
    "all_domains": false
  },
  "schema_version": 3
}
```

:::tip
Este exemplo utiliza o **endpoint Apache Kafka**, mas você pode alterar o `endpoint_type` para qualquer um dos endpoints disponíveis e adicionar os campos obrigatórios específicos. Consulte os endpoints disponíveis e seus campos na tabela [Data Stream Endpoints](https://api.azion.com/#b9aac0c7-173a-462b-87f6-a18d6558d3e5).
:::

Aguarde alguns minutos para que as alterações se propaguem e seu Data Stream será criado.

:::tip
Consulte a [documentação da API da Azion](https://api.azion.com/) e a [especificação OpenAPI](https://github.com/aziontech/azionapi-openapi/) para saber mais sobre o que a API da Azion pode oferecer.
:::
<br /><br />
---

**Contributors** <ContributorList>Contributor</ContributorList>
