---
title: Como utilizar o Google BigQuery para receber dados do Data Streaming
description: Hands-on para configurar o Google BigQuery para receber dados do Data Streaming.
meta_tags: data streaming, connector, endpoint, google, bigquery
namespace: documentation_how_to_configurations_google_bigquery
permalink: /documentacao/produtos/guias/google-bigquery-endpoint/
menu_namespace: observeMenu
---

Ao configurar [data streamings](/pt-br/documentacao/produtos/data-streaming/), você precisa configurar um endpoint específico para transmitir seus dados da Azion.

Continue lendo para obter o passo a passo sobre como conectar um endpoint [Google BigQuery](https://cloud.google.com/bigquery/) com o Data Streaming.


---

## Pré-requisitos do Google BigQuery

Para começar com **Google BigQuery**, siga estes passos:

1. Crie [uma conta na Google Cloud Platform](https://cloud.google.com/).
2. Crie [um projeto](https://developers.google.com/workspace/guides/create-project) na Google Cloud Platform. O **Project ID** será dado enquanto você o cria, salve-o para usar depois.
3. Crie uma [service account](https://cloud.google.com/iam/docs/service-accounts-create) na Google Platform.
4. A service account deve possuir as permissões de **BigQuery Admin**. Selecione essa opção no menu suspenso do campo **Role**.

> Para mais detalhes sobre as permissões padrões atribuídas à função do editor de dados BigQuery, acesse [BigQuery Roles](https://cloud.google.com/iam/docs/understanding-roles#bigquery-roles).

Você também precisa criar e salvar as seguintes informações:

- Habilitar a BigQuery API
- Um dataset
- Uma service account key

### Habilitar BigQuery API

Agora, você deve acessar o [API ⁠Manager](https://console.cloud.google.com/apis/library) e habilitar a BigQuery API.

A BigQuery API tem suporte a um endpoint para transmitir linhas em uma tabela. No entanto, esse endpoint não é suportado na versão *Free Tier*. Para utilizá-lo, é necessário habilitar a versão completa da plataforma com a configuração de faturamento (billing).

Veja mais detalhes sobre esse passo na [documentação de gerenciamento de faturamento de projetos](https://cloud.google.com/billing/docs/how-to/modify-project). As taxas de utilização da API podem ser consultadas na tabela de preços na seção **Streaming Inserts**.

### Criar dataset

Após habilitar a API, você precisa criar um dataset. Para isso, você precisa criar um projeto no [console da Google Cloud](https://console.cloud.google.com/bigquery). Por padrão, o BigQuery já vem habilitado em novos projetos.

Após criar o projeto, siga os seguintes passos:

1. No seu Google Cloud console, abra a página do BigQuery.
2. Selecione o projeto no qual você quer criar o dataset.
3. Clique na opção **Actions**, com a elipse vertical >⁠ **Create dataset**.
4. Preencha as [informações solicitadas](https://cloud.google.com/bigquery/docs/datasets#create-dataset). Após escolher um **Dataset ID**, salve-o para usar depois.
5. Clique em **Create dataset**.

Após criar o dataset, você deve criar uma tabela:

1. [Crie uma tabela](https://cloud.google.com/bigquery/docs/tables) e associe-a ao dataset que você acabou de criar.
2. Salve o **Table name**, o nome da tabela, que você escolheu. Esse é o ⁠Table ID.
3. Em **Schema**, adicione a estrutura dos dados que serão inseridos.
4. Clique em **Create table**.

Após a tabela ser criada, é possível fazer a ingestão de dados através da BigQuery API.

### Criar service account key

Agora, você deve criar uma *service account key*, uma chave de conta de serviços, para continuar sua configuração.

1. Após criar sua service account, acesse-a.
2. No menu esquerdo, clique em **IAM & Admin** > **Service Accounts**.
3. Selecione a service account que você criou na lista.
4. Clique na aba **KEYS** no menu superior > **ADD ⁠KEY** > **Create new Key**.
5. Em **Key Type**, escolha a opção **JSON** > clique em **CREATE**.
6. Após a confirmação, o arquivo JSON com as credenciais será baixado automaticamente.

O conteúdo do arquivo deve ser similar ao exemplo:

```json
{
  "type": "service_account",
  "project_id": "azion-data",
  "private_key_id": "13e018d99d6ay9e3c9f3e21a7a7e0226a1ae082",
  "private_key": "-----BEGIN PRIVATE KEY-----\\nxxx\\n-----END PRIVATE KEY-----\\n",
  "client_email": "myemail@azion.com",
   ...
}
```

---

## Configurar o novo endpoint no Data Streaming

Agora, siga os próximos passos para configurar o novo endpoint criado no **Google BigQuery** no seu Azion **Data Streaming**.

Você encontra os passos detalhados para a configuração completa no guia [Como utilizar o Data Streaming](/pt-br/documentacao/produtos/guias/como-usar-data-streaming/).

Nas configurações de **Destination**:

1. No menu suspenso **Endpoint Type**, selecione **Google BigQuery**.
2. Em **Project ID**, adicione a ID do seu projeto na Google Cloud.
3. Em **Data Set ID**, adicione a ID do seu dataset criado no Google BigQuery.
4. Em **Table ID**, adicione a ID da sua tabela que irá receber os dados transmitidos.
5. Em **Service Account Key**, copie o conteúdo do arquivo `JSON` baixado que contém sua chave de acesso privado.

A chave de acesso privada deve ser similar ao exemplo:

```json
{
	"type": "service_account",
	"project_id": "[name]",
	"private_key_id": "13c73d892hf6e8s04hjkloi6759f1e6df39f9038",
	"private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvAIpDEryaqLPEuiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCzL+bgfcynhWOx\nAKQ6wfsnwl/jEYsu5KxlPTtr11hmHLVtDAC68FVjAL029zfTjCRIG9d2ttm6fySY\nJm7Y1MwpahekDmFhMbISxA5UfN0KAF5Bs/uGU6hm17tq+ZDSA1L9f3UIvAJ5/cqu\n9CKhU1Dm1TChL8nxIfAb90G7ga6QJVve3ko/0KHpq7pdm3tp6VsVQ+fgwKNi7L+A\n4CvHFT0jX4jRDIFUKePRuxyleZV5p1Y3BHSLCIC1X+oe36a0RMLBCrWVdhHwAqBb\nbec3NYTen4Re+BidL0cfJ8IsVhjdWuibQTaT2/V+OzA+JgzXvpYSI0jWUvUiYtRK\njxz7AGaXAgMBAAECggEASA0bua76ElAjTg9ixKFg7u0/4P4cWfAM1cf64+e9zPJ6/\nH5NaW8cpWf+7C/MxlOdH/zojHKScMyWhXu0wvpKalGXWr+F5/mVCsu2wqfoIhPh\nzeAq72KB5MtBLI4ecPkbCnyGKbt9909TfRrrLBEl58EHNaUwEvRDzsmBpn1JDe75\nJ2ODNf714DsDtghG5Jy5nZ75Bk6ny5mYp67q6IdCUFJeLgJUwfNdtUJmcQ5x7lw3\nujR0vEyEWXpiSAsIhIi0XgMr5NSbBdH+e+P9gVUZwqtRbshdH6aPalIxh1rhdEtY\nJguGzK9nbYQtzm0Mdka3VZtUZIEQAqlg8OZe8xLpa+p392TU64sQlrJxQMZxPNtU\ntPuDwtDAgmwGZNGFxgBFIMuzN88QpL5zPFSBbJoHt5xJ3sGNmeuDF9SrBXNrFz\n9hmqUtoUa0iNheVNG+Y7smEnJNjuSYldAlBQ5qjqSr1IAJTwoUE0fF1P3SbFK9b2\nW6TJ73gqF78EQIJf6t3kOczm/QKB0pRMSuGK2ga45ig2CtMSklUHVjL3A+zcEP9NH\nosFRYkxZZShPqKj2j0PAdB2TcUgrl1a+I+6oA1oU/j0fuJiux9pxrz9I8QfTVwJQS\n/oCcHsKMrDngi0+DkETHDe9peDPTfO4MAh+G285MDPa3LegEG2iVGsqhp+5v8Jdm0Vl\nCyZQJ526IwKBgESw1npFyakE0sMGjlwBRjworH5HjajNPsJjZtspaU7TkCXsS7bt\nwFmLmm7205SKM+1N9C4owSn25uxIWbsb/wB6iuK+EyP+K3qnjPI/GsVRpDjXb1Ma\niBe4tZCUUP/lJGj8HvBk+kD/lQoFuFndD6cvwDze+PpUeN2oe7IiiZQBlAoGAcQUp\nHT3lCVmxXC049FKa8DyWTJIQJhkJmDADeqlYaCFaUe9YC490Y+BtYZHX0UNDXCnFZ\nLIBTtRTPfFU02kUBAcGn0ALc74QwUnJlImvuOeYOlgGwy6QzcRQ6dtfsDWROwKk\nNCAAjYBylKF2QcuZC3rwe0qN5EIe/0DoFmWUD7ELCgYBIKy2ojKY2d+IByJakBOXt\nojwlCj+I5GpDtDeVhzw9u+74j7KoLsKE057DnMGgouGdVH2xCKih7E71iDKPx1Li\nar9Dz3LsPzHGYXt0LBa+0RBm8mRVb68AlFuN3XJ7g9H8tXPZl38hwLKM\EkDJruapG84nuOcgrp2zGHwYtp9S7DfUg==\n-----END PRIVATE KEY-----\n",
	"client_email": "email@myemailaccount.com",
	"client_id": "1835090363570189530221",
	"auth_uri": "https://accounts.google.com/o/oauth2/auth",
	"token_uri": "https://oauth2.googleapis.com/token",
	"auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
	"client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/mytest.com"
}
```



6. Confira se o switch **Active** está habilitado.
7. Clique no botão **Save**.

Toda autenticação com o Google Oauth2 e geração de JWTokens será realizada pelos sistemas de backend do Data Streaming.

Após salvar as configurações, seus dados serão enviados para o novo endpoint configurado.

Você pode acompanhar as chamadas feitas pelo Data⁠ Streaming ao Google BigQuery no [Real-Time Events](/pt-br/documentacao/produtos/real-time-events/). Para isso, selecione **Data Source** > **Data Streaming** e escolha as opções de filtro que você deseja utilizar.

---

### Marca registrada

[Google BigQuery](https://cloud.google.com/bigquery) e [Google Cloud Platform](https://cloud.google.com/) são marcas registradas de Google LLC nos Estados Unidos e/ou outros países.




---

import ContributorList from '~/components/ContributorList.astro'

**Contribuidores** <ContributorList>Contributor</ContributorList>
