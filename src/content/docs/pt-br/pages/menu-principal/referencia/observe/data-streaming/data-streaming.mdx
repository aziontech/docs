---
title: Data Stream
description: >-
  Data Stream é um produto de Observe que permite que você alimente suas
  plataformas de stream, SIEM e big data com os logs de eventos de suas
  aplicações na Azion em tempo real.
meta_tags: >-
  data, streaming, edge computing, stream, observe, observability,
  observabilidade, logs, events, eventos
namespace: documentation_products_data_streaming
permalink: /documentacao/produtos/observe/data-stream/
---

import Tag from 'primevue/tag';
import LinkButton from 'azion-webkit/linkbutton'

**Data Stream** é um produto de [Observe](/pt-br/documentacao/produtos/observe/) que permite que você alimente suas plataformas de stream, SIEM e big data com os logs de eventos de suas aplicações na Azion em tempo real. Para manter uma performance avançada, usa a codificação ASCII para evitar problemas de parser e problemas na interpretação dos dados.

O Data Stream é responsável por coletar, agrupar e transmitir logs brutos para o destino escolhido, oferecendo flexibilidade para a integração de dados. Você pode escolher de quais produtos e domínios disponíveis na Azion deseja coletar seus dados de logs e conectá-los ao endpoint de sua plataforma de análise de dados. Você também pode escolher quais variáveis quer usar em sua análise.

Ao acessar o Data Stream por meio do [Azion Console](https://console.azion.com/data-stream) ou da [Azion API](https://api.azion.com/#04257075-3691-4b5f-b22b-22a5b6653f3f), você pode realizar diversas ações para gerenciar seus fluxos de dados, incluindo:

- Manter um conjunto organizado de logs.  
- [Criar um novo data stream](/pt-br/documentacao/produtos/guias/como-usar-data-stream/).  
- Conectar seus data streams a [endpoints](/pt-br/documentacao/produtos/guias/#observe).  
- [Editar as configurações de um data stream](/pt-br/documentacao/produtos/guias/como-usar-data-stream/) diretamente da lista ao selecionar o stream que deseja atualizar.  
- [Visualizar informações detalhadas](/pt-br/documentacao/produtos/observe/troubleshoot/data-stream-monitorar-metricas/) sobre cada stream para garantir que tudo esteja funcionando corretamente.  
- [Excluir ou parar data streams](/pt-br/documentacao/produtos/guias/observe/deletar-data-stream/) que não são mais necessários para otimizar seu sistema.
- Compreender o comportamento dos seus usuários.  
- Analisar o desempenho do seu conteúdo e das suas aplicações.  
- Identificar dados relacionados a ameaças de segurança.  
- Tomar decisões informadas.  
- Melhorar suas aplicações e seu negócio por meio de práticas confiáveis de observabilidade.  

Após configurar o seu Data Stream, você pode verificar o envio bem-sucedido dos seus logs por meio do [Real-Time Events](/pt-br/documentacao/produtos/observe/real-time-events/).

:::tip  
Visite a [página de Guias](/pt-br/documentacao/produtos/guias/#observe) e a [seção Observe](/pt-br/documentacao/produtos/observe/visao-geral/) para explorar mais opções de gerenciamento dos seus data streams.  
:::

---

## Implementação

| Tarefa | Guia |
| ---- | ----- |
| Acessar o Data Stream | [Primeiros passos do Data Stream](/pt-br/documentacao/produtos/observe/data-stream/primeiros-passos/) |
| Configurar o Data Stream | [Como configurar as main settings do Data Stream](/pt-br/documentacao/produtos/guias/como-usar-data-stream/) |
| Parar ou deletar um data stream | [Como parar ou deletar um data stream](/pt-br/documentacao/produtos/guias/observe/deletar-data-stream/) |
| Associar domínios | [Como associar domínios no Data Stream](/pt-br/documentacao/produtos/guias/data-stream-associar-dominios/) |
| Criar template personalizado | [Como criar um template personalizado no Data Stream](/pt-br/documentacao/produtos/guias/data-stream-template-personalizado/) |

---

## Como gerenciar o Data Stream

O Data Stream permite que você crie, edite, visualize e exclua configurações de data stream.

Você pode [criar um novo data stream](/pt-br/documentacao/produtos/guias/como-usar-data-stream/) através do [Azion Console](https://console.azion.com/data-stream):

1. Acesse **Products menu** > **Data Stream**.
2. Clique no botão **+ Stream**.
3. Complete a configuração.
4. Clique no botão **Save**.

Para [editar qualquer stream](/pt-br/documentacao/produtos/guias/como-usar-data-stream/), acesse a página do Data Stream e selecione o stream com o qual deseja trabalhar. Edite as configurações conforme desejado e salve as alterações.

Se você deseja [parar um data stream](/pt-br/documentacao/produtos/guias/observe/deletar-data-stream/) em sua conta Azion, siga estas etapas:

1. Acesse o Azion Console > **Data Stream**.
2. Na lista, selecione o stream que deseja parar.
3. No lado direito da linha, você pode ver uma label indicando o status do stream: Active ou Inactive.
4. Na página de configuração, navegue até a seção **Status**.
5. Desative o switch para parar o data streaming.
6. Clique no botão **Save**.

Para ativar o streaming novamente, repita este processo, ativando o switch na seção **Status**.

Para [excluir um stream](/pt-br/documentacao/produtos/guias/observe/deletar-data-stream/): acesse a página Data Stream, navegue até o lado direito da linha e clique no ícone de lixeira para o stream que deseja excluir.

---

## Logs

Por padrão, o **Data Stream** envia seus logs de eventos quando o bloco com as variáveis atinge *2.000 registros*, a cada *60 segundos*, ou quando o tamanho do pacote atinge o valor determinado no *campo Max Size do payload*, o que ocorrer primeiro. No entanto, se você estiver usando o endpoint AWS Kinesis Data Firehose, o Data Stream enviará seus logs de eventos quando o bloco atingir *500 registros* ou a cada *60 segundos*.

Por exemplo, se um bloco atinge 2.000 registros em 13 seconds, ele é enviado.

Ao ativar um stream, ocorre um tempo de propagação até que os logs fiquem disponíveis para consulta em ferramentas ou produtos como o **Real-Time Events**.

Para informações detalhadas sobre cada seção de configuração do Data Stream, continue lendo as próximas subseções.

---

## Data sources

Um **Data Source** representa a aplicação na Azion que gera os registros de eventos que você quer usar. Ao selecionar um data source, você decide de onde seus dados vão ser coletados e as demais configurações são feitas de acordo com sua escolha.

Selecionar um data source na lista suspensa é obrigatório. Você pode escolher entre:

- [Activity History](#activity-history)
- [Applications](#applications)
- [Functions](#functions)
- [WAF Events](#waf-events)

Cada data source tem *variáveis* pré-configuradas, combinadas em um template, que representam a informação específica que você pode receber dos seus registros de eventos. Veja os pré-requisitos e as variáveis de cada data source e que dados fornecem a seguir.

### Activity History

Não é possível associar domínios ao usar o data source **Activity History**.

O data source **Activity History** exibe os dados referentes aos [registros de atividade](/pt-br/documentacao/produtos/gestao-de-contas/activity-history/) de sua conta no Azion Console. As seguintes variáveis estão disponíveis para essa opção:

| Variável | Descrição |
| --- | --- |
| $author_email | Email do usuário do Azion Console que executou a atividade. |
| $author_name | Nome do usuário do Azion Console que executou a atividade. |
| $client | Identificador único de cliente Azion. Exemplo: 4529r |
| $comment | Campo em branco disponível para que usuários possam adicionar comentários ao realizarem mudanças. |
| $time | Data e hora da requisição. Exemplo: Oct. 31st, 2022 - 19:30:41 |
| $title | Título da atividade, composto por: nome do modelo, nome e tipo de atividade. Exemplo: Pathorigin Default Origin was changed |
| $type | Tipo de ação executada no Azion Console: CREATED, CHANGED, DELETED ou SIGNED UP. |

---

### Applications

O data source **Applications** exibe os dados das requisições feitas para suas [applications](/pt-br/documentacao/produtos/build/applications/) na Azion. As seguintes variáveis estão disponíveis para essa opção:

| Variável | Descrição |
| --- | --- |
| $asn | Autonomous System Number Allocation (ASN), que são redes de endereços IP gerenciadas por uma ou mais operadoras de rede que têm uma política de roteamento clara e única. Exemplo: AS52580 |
| $bytes_sent | Número de bytes enviados para o cliente. Exemplo: 191 |
| $client | Identificador único de cliente Azion. Exemplo: 4529r |
| $configuration | Identificador único de configuração Azion definido no arquivo de configuração do virtual host. Exemplo: 1595368520 |
| $country | País do cliente detectado pela geolocalização de endereço IP. Exemplo: United States |
| $host | Informação de host enviada na linha da requisição. Armazena: nome do host da linha da requisição, *ou* o nome do host do campo *Host* do campo host do cabeçalho, *ou* o nome do servidor correspondente à requisição. |
| $http_referrer | Endereço da página na qual o usuário fez a requisição. Valor do cabeçalho Referer. Exemplo: https://example.com |
| $http_user_agent | Identificação da aplicação, do sistema operacional, do fornecedor, e/ou da versão do usuário final. Valor do cabeçalho User-Agent. Exemplo: Mozilla/5.0 (Windows NT 10.0; Win64; x64) |
| $proxy_status | Código de status de erro HTTP ou da origem quando nenhuma resposta é obtida da origem. Exemplo: 520. **Em caso de cache, a resposta é `-`**. |
| $remote_addr | Endereço IP da origem que gerou a requisição. |
| $remote_port | Porta remota da origem que gerou a requisição. |
| $request_id | Identificador único da requisição. Exemplo: 5f222ae5938482c32a822dbf15e19f0f |
| $request_length | Tamanho da requisição, incluindo a linha da requisição, cabeçalhos e corpo. |
| $request_method | Método da requisição. Exemplo: GET ou POST |
| $request_time | Tempo de processamento da requisição, em segundos, desde que os primeiros bytes foram lidos a partir do cliente. Exemplo: `0.234`|
| $request_uri | URI da requisição realizada pelo usuário, sem a informação do host e de protocolo e com argumentos. Exemplo: /v1?v=bo%20dim |
| $requestPath | URI da requisição sem a informação de Query String, Host e Protocol. Exemplo:  se `request_uri`: /jira/plans/48/scenarios/27?vop=320#plan/backlog, então `requestPath`: /jira/plans/48/scenarios/27 |
| $requestQuery | Parâmetros da URI da requisição. Exemplo: requestQuery: vid=320#plan/backlog |
| $scheme | Esquema da requisição. Exemplo: HTTP ou HTTPS |
| $sent_http_content_type | Cabeçalho “Content-Type” enviado na resposta da origem. Exemplo: text/html; charset=UTF-8. |
| $sent_http_x_original_image_size | Cabeçalho "X-Original-Image-Size" enviado na resposta da origem. Informa o tamanho da imagem original. Exemplo: 987390 |
| $server_addr | Endereço IP do servidor que recebeu a requisição. |
| $server_port | Porta remota do servidor que recebeu a requisição. |
| $server_protocol | Protocolo da requisição. Exemplo: `HTTP/1.1`, `HTTP/2.0`, `HTTP/3.0` |
| $session_id | Identificação da sessão. |
| $ssl_cipher | String de cifra utilizada para estabelecimento de conexão TLS. Exemplo: TLS_AES_256_GCM_SHA384 |
| $ssl_protocol | Protocolo de uma conexão TLS estabelecida. Exemplo: TLS v1.2 |
| $ssl_server_name | Nome do servidor, informado pelo cliente, que o cliente está tentando conectar. Exemplo: www.example.com |
| $ssl_session_reused | Retorna `r` se a sessão TLS for reutilizada; nos demais casos, retorna `.`. |
| $state | Estado do cliente detectado pela geolocalização de endereço IP. Exemplo: CA |
| $status | Código de status HTTP da requisição. Exemplo: 200 |
| $stream | ID definida através de configuração de host virtual com base na diretiva de localização. Definido no arquivo de configuração do host virtual. |
| $tcpinfo_rtt | Tempo de ida e volta (RTT), em microsegundos, medido pelo edge para o usuário. Disponível em sistemas que suportam a opção de socket TCP_INFO. Exemplo: `72052` |
| $time | Data e hora da requisição. Exemplo: Oct. 31st, 2022 - 19:30:41 |
| $traceback | Informa os nomes dos Rules Engine da Application e do Firewall executadas pela requisição. |
| $upstream_addr | Endereço IP e porta do cliente. Também pode armazenar múltiplos servidores ou grupos de servidores. Exemplo: 192.168.1.1:80. Quando a resposta é `127.0.0.1:1666`, o upstream é o Azion [Cells Runtime](/pt-br/documentacao/runtime/visao-geral/). |
| $upstream_bytes_received | Número de bytes recebidos pelo edge da origem, se o conteúdo não estiver em cache. Exemplo: 8304 |
| $upstream_bytes_sent | Número de bytes enviados para a origem. Exemplo: 2733 |
| $upstream_cache_status | Status do cache local do edge. Exemplo: MISS, BYPASS, EXPIRED, STALE, UPDATING, REVALIDATED ou HIT. |
| $upstream_connect_time | Tempo que leva para o edge estabelecer uma conexão com a origem, em segundos. No caso de TLS, inclui o tempo gasto no handshake. Exemplo: `0.123`. **Retorna `0` para KeepAlive e `-` para cache** |
| $upstream_header_time | Tempo para que o edge receba os cabeçalhos de resposta da origem, em segundos. Exemplo: `0.345`. **No caso de cache, a resposta é `-`** |
| $upstream_response_time | Tempo para o edge receber uma resposta padrão da origem, em segundos, incluindo cabeçalhos e corpo. Exemplo: `0.876`. **No caso de cache, a resposta é `-`.** |
| $upstream_status | Código de status HTTP da origem. Se um servidor não pode ser selecionado, a variável mantém o código de status 502 (Bad Gateway). **No caso de cache, a resposta é `-`** |
| $waf_attack_action | Informa a ação do WAF em relação à ação. Pode ser: $BLOCK, $PASS, $LEARNING_BLOCK ou $LEARNING_PASS. |
| $waf_attack_family | Informa a classificação da infração de WAF detectada na requisição. Exemplo: SQL, XSS, TRAVERSAL, entre outras. |
| $waf_block | Informa se o WAF bloqueou ou não a ação. `0` quando não bloqueado e `1` quando bloqueado. Quando em `Learning Mode`, ele não será bloqueado, independentemente do retorno. |
| $waf_headers | Quando os cabeçalhos de solicitação enviados pelo usuário são analisados pelo módulo WAF e marcados como **blocked**, `$waf_block = 1`. Tem uma string codificada em base64. Caso contrário, ele terá um traço `-`. Aplica-se aos modos `WAF Learning` ou `Blocking`. |
| $waf_learning | Informa se o WAF está em Learning Mode. Pode ser `0` ou `1`. |
| $waf_match | Lista de infrações encontradas na requisição do usuário final. É formada por elementos chave-valor: a *chave* é referente ao tipo de infração detectada; o *valor* apresenta a string que gerou a infração. |
| $waf_score | Informa a pontuação que será incrementada em caso de match com as regras criadas para o WAF. |
| $waf_total_blocked | Informa o número total de requisições bloqueadas. |
| $waf_total_processed | Informa o número total de requisições processadas. |

:::note[nota]
Você pode adicionar a variável `$traceback` em um custom template com o data source Applications se você tem ativada a opção de *Debug rules* em sua aplicação. Veja mais em [Debugging rules em Applications](/pt-br/documentacao/produtos/build/applications/rules-engine/#debugging-rules).
:::

As variáveis: `$upstream_bytes_received`; `$upstream_cache_status`; `$upstream_connect_time`; `$upstream_header_time`; `$upstream_response_time` e `$upstream_status` podem apresentar *mais de um elemento separado por vírgula*. Quando uma conexão é disparada, seja por redirecionamento interno ou escolha de origem com [Load Balancer](/pt-br/documentacao/produtos/secure/connectors/load-balancer/), por exemplo, cada valor contido no campo representa a respectiva conexão iniciada. O campo pode ser separado por:

- Uma vírgula, representando múltiplos IPs.
- Dois-pontos, representando redirecionamento interno.

Se vários servidores foram contatados durante o processamento da solicitação, seus endereços são separados por vírgulas, por exemplo: `192.168.1.1:80, 192.168.1.2:80`.

Se ocorre um redirecionamento interno de um grupo de servidores para outro, iniciado por *X-Accel-Redirect* ou *Error Responses*, os endereços de servidores de diferentes grupos são separados por *dois-pontos*. Exemplo: `192.168.1.1:80, 192.168.1.2:80, unix:/tmp/sock : 192.168.10.1:80, 192.168.10.2:80`.

Se um servidor não puder ser selecionado, a variável mantém o nome do grupo de servidores.

Considerando vários valores como transições na conexão, o último valor tende a ser o mais importante. Se você usa o recurso *Error Responses* nas suas applications, você verá dois valores nos campos *upstream* que representam o status da origem e o resultado da solicitação feita para que o conteúdo seja entregue. Em casos normais, você terá `502: 200`.

*502* é o código de erro HTTP para a resposta da primeira tentativa de obter conteúdo do servidor de origem. Como ele retornou um erro *502*, considerando que você configurou uma *Error Responses* para o status *502*, outra solicitação será feita para que o URI seja definido. Em seguida, a página será entregue e o status HTTP será adicionado aos campos *upstream*, respeitando sua posição para todos eles. Neste exemplo, o resultado é a composição `502 : 200`.

---

### Functions

Requer: <Tag severity="info" client:only="vue">Functions</Tag>

O data source **Functions** exibe os dados referentes às solicitações feitas para suas [functions](/pt-br/documentacao/produtos/build/applications/functions/) na Azion.

As seguintes variáveis estão disponíveis para essa opção:

| Variável | Descrição |
| --- | --- |
| $client | Identificador único de cliente Azion. Exemplo: 4529r |
| $edge_function_id | Identificador da Function. Exemplo: 1321 |
| $global_id | Identificador da configuração. |
| $log_level | Nível do log gerado: ERROR, WARN, INFO, DEBUG ou TRACE. |
| $log_message | Mensagem editável usada para o log na chamada da função. Disponível para usuários identificarem e reportarem acontecimentos. |
| $message_source | Fonte da mensagem. Quando as mensagens são geradas pela API do Console: `CONSOLE`; quando são relacionadas a uma mensagem de erro: `RUNTIME`.  |
| $request_id | Identificador único da requisição. Exemplo: 5f222ae5938482c32a822dbf15e19f0f |
| $time | Data e hora da requisição. Exemplo: Oct. 31st, 2022 - 19:30:41 |

---

### WAF Events

Requer: <Tag severity="info" client:only="vue">Web Application Firewall</Tag>

O data source **WAF Events** exibe os dados referentes às requisições analisadas pelo [Web Application Firewall (WAF)](/pt-br/documentacao/produtos/secure/firewall/web-application-firewall/) para que você possa mapear a pontuação atribuída à requisição, às regras de WAF que deram match e aos motivos do bloqueio.

As seguintes variáveis estão disponíveis para essa opção:

| Variável | Descrição |
| --- | --- |
| $blocked | Informa se o WAF bloqueou ou não a ação. `0` quando não bloqueado e `1` quando bloqueado. Quando em `Learning Mode`, ele não será bloqueado, independentemente do retorno. |
| $client | Identificador único de cliente Azion. Exemplo: 4529r |
| $configuration | Identificador único de configuração Azion definido no arquivo de configuração do virtual host. Exemplo: 1595368520 |
| $country | País do cliente detectado pela geolocalização de endereço IP. Exemplo: United States |
| $headers | Quando os cabeçalhos de solicitação enviados pelo usuário são analisados pelo módulo WAF e marcados como **blocked**, `$waf_block = 1`. Tem uma string codificada em base64. Caso contrário, ele terá um traço `-`. Aplica-se aos modos `WAF Learning` ou `Blocking`. |
| $host | Informação de host enviada na linha da requisição. Armazena: nome do host da linha da requisição, *ou* o nome do host do campo *Host* do campo host do cabeçalho, *ou* o nome do servidor correspondente à requisição. |
| $remote_addr | Endereço IP da origem que gerou a requisição. |
| $requestPath | URI da requisição sem a informação de Query String, Host e Protocol. Exemplo: se `request_uri`: /jira/plans/48/scenarios/27?vop=320#plan/backlog, então `requestPath`: /jira/plans/48/scenarios/27 |
| $requestQuery | Parâmetros da URI da requisição. Exemplo: requestQuery: vid=320#plan/backlog |
| $server_protocol | Protocolo da requisição. Exemplo: `HTTP/1.1`, `HTTP/2.0`, `HTTP/3.0` |
| $time | Data e hora da requisição. Exemplo: Oct. 31st, 2022 - 19:30:41 |
| $truncated_body | Essa variável foi descontinuada. Portanto, não terá um valor atribuído à ela, apenas `-` no lugar de um valor. |
| $version | A versão de Azion Log usada. Exemplo: v5 |
| $waf_args | Argumentos da requisição. |
| $waf_attack_action | Informa a ação do WAF em relação à ação. Pode ser: $BLOCK, $PASS, $LEARNING_BLOCK ou $LEARNING_PASS. |
| $waf_attack_family | Informa a classificação da infração de WAF detectada na requisição. Exemplo: SQL, XSS, TRAVERSAL, entre outras. |
| $waf_learning | Informa se o WAF está em modo Learning. Pode ser `0` ou `1`.  |
| $waf_match | Lista de infrações encontradas na requisição do usuário final. É formada por elementos chave-valor: a *chave* é referente ao tipo de infração detectada; o *valor* apresenta a string que gerou a infração. |
| $waf_score | Informa a pontuação que será incrementada em caso de match com as regras criadas para o WAF. |
| $waf_server | Hostname usado na requisição de WAF. Exemplo: api-login.azion.com.br |
| $waf_uri | URI usada na requisição do WAF. Exemplo: /access/v2/after-login |

---

## Templates
Um template no **Data Stream** fornece o conjunto de variáveis pré-configuradas disponíveis para cada data source em um formato adequado para a transferência dos seus registros de eventos. Após selecionar o seu data source, você pode:

- Selecionar o template correspondente, fornecido pela Azion.
- Personalizar seu próprio template, escolhendo quais variáveis quer usar.

Você encontra quatro templates fornecidos pela Azion e um **Custom Template**, que permite que você decida quais variáveis serão usadas. Os templates estão disponíveis na lista suspensa **Template** e as variáveis para os templates são exibidas no campo de código **Data Set** em formato JSON.

Veja qual template corresponde a qual data source:

| Data Source | Template |
| --- | --- |
| Activity History | Activity History Collector |
| Applications | Applications + WAF Event Collector |
| Functions | Functions Event Collector |
| WAF Events | WAF Event Collector |
| Todos | Custom Template |

Ao selecionar um dos templates fornecidos pela Azion, você não consegue modificar as variáveis exibidas no campo de código **Data Set**. Ao selecionar **Custom Template**, você consegue personalizar quais variáveis quer usar de acordo com suas necessidades.

<LinkButton severity="secondary" link="/pt-br/documentacao/produtos/guias/data-stream-template-personalizado/" label="Como criar um template personalizado" />

---

## Domínios

Você pode associar os seus [domínios existentes](/pt-br/documentacao/produtos/build/applications/domains/) registrados na Azion ao seu stream. Se você ainda não tem um domínio registrado na sua conta, veja a documentação sobre [Criar um novo domínio associado a sua application](/pt-br/documentacao/produtos/guias/configurar-dominio/).

Ao associar um domínio, os eventos relacionados a esse ou a esses domínios específicos são coletados e enviados para seu endpoint. Você pode associar um ou mais domínios e você tem a opção de filtrar domínios com **Filter Domains** ou selecionar todos os domínios com **All Domains**.

Ao selecionar **All Domains**, a plataforma seleciona automaticamente todos os domínios atuais e *futuros* de sua conta no Azion Console.

<LinkButton severity="secondary" link="/pt-br/documentacao/produtos/guias/data-stream-associar-dominios/" label="Como associar domínios" />

Se você selecionar a opção **All Domains**, você também pode configurar a porcentagem de dados que quer receber, de forma aleatória, do seu stream através da opção de *Sampling*. Além de filtrar por amostra, a opção também reduz custos de coleta e análise de dados.

:::note[nota]
No momento, a opção *Sampling* não está disponível para todas as contas do Azion Console. Se você deseja usá-la em sua conta, [contate o time de vendas](https://www.azion.com/pt-br/contate-vendas/).
:::

O campo **Sampling (%)** deve conter a porcentagem de dados que quer receber. Essa porcentagem retornará dados relacionados a todos os seus domínios.

Quando a opção de Sampling está habilitada, você só pode adicionar *um* stream em sua conta. Após esse stream ser desabilitado, a opção **Add Streaming** é habilitada novamente na tela do Data Stream no Azion Console.

---

## Endpoints

O *endpoint* é o destino para onde você quer enviar os dados coletados pela Azion, que costuma ser uma plataforma de processamento de stream ou uma ferramenta de análise que você já usa. O tipo de endpoint representa o método que você quer configurar como destino para os seus dados.

A Azion suporta os seguintes endpoints:

- [Implementação](#implementação)
- [Como gerenciar o Data Stream](#como-gerenciar-o-data-stream)
- [Logs](#logs)
- [Data sources](#data-sources)
  - [Activity History](#activity-history)
  - [Applications](#applications)
  - [Functions](#functions)
  - [WAF Events](#waf-events)
- [Templates](#templates)
- [Domínios](#domínios)
- [Endpoints](#endpoints)
  - [Apache Kafka](#apache-kafka)
  - [AWS Kinesis Data Firehose](#aws-kinesis-data-firehose)
  - [Azure Blob Storage](#azure-blob-storage)
  - [Azure Monitor](#azure-monitor)
  - [Datadog](#datadog)
  - [Elasticsearch](#elasticsearch)
  - [Google BigQuery](#google-bigquery)
  - [IBM QRadar](#ibm-qradar)
  - [S3 - Simple Storage Service](#s3---simple-storage-service)
  - [Splunk](#splunk)
  - [Standard HTTP/HTTPS POST](#standard-httphttps-post)
- [Payload](#payload)
  - [Payload personalizado com Activity History](#payload-personalizado-com-activity-history)
- [Tratamento de erros](#tratamento-de-erros)
  - [Comportamento do status HTTP 504](#comportamento-do-status-http-504)
  - [Comportamento do status HTTP 503](#comportamento-do-status-http-503)
- [Limites](#limites)
- [](#)

:::tip[dica]
Não encontrou nesta lista o conector de endpoint que você está procurando? Participe de uma pesquisa rápida sobre [conectores do Data Stream](https://forms.gle/jhUTEsoqbdn174j59) e compartilhe quais você usa e gostaria de encontrar aqui.
:::

Para configurar um endpoint, você deve selecionar um **Endpoint Type** na lista suspensa do Azion Console. Em seguida, você deve preencher os campos apresentados de acordo com sua escolha de tipo de endpoint.

Veja mais sobre cada um dos endpoints disponíveis e seus campos a seguir. Campos marcados com asterisco `*` no Azion Console são obrigatórios.

---

### Apache Kafka

Para configurar o endpoint [Apache Kafka](https://kafka.apache.org/quickstart), você deve ter acesso à plataforma para obter as seguintes informações:

- **Bootstrap Servers**: os servidores — hosts e portas — no cluster do Kafka. Você pode adicionar um ou mais servidores Kafka separando-os com vírgula `,` e *sem espaços*. Devem estar neste formato: `myownhost.com:2021,imaginaryhost.com:4525,anotherhost:4030`.

Não é necessário incluir todos os servidores de seu cluster nesse campo, mas apenas alguns dos servidores que serão usados para a conexão inicial.

:::tip[dica]
É recomendado que você use mais de um servidor para aumentar a redundância e a disponibilidade.
:::

- **Kafka Topic**: nome do tópico do seu cluster Kafka para o qual o Data Stream deve enviar mensagens. Este campo aceita apenas *um* tópico. Exemplo: `azure.analytics.fct.pageviews.0`

- **Transport Layer Security (TLS)**: opção para fazer envios criptografados usando o *Transport Layer Security (TLS)*. O TLS é um protocolo criptográfico projetado para fornecer segurança de comunicação em uma rede de computadores e é amplamente usado como uma camada de segurança de HTTP. Para usar a opção, selecione **Yes**.

Se você deseja usar esse protocolo, certifique-se de que o endpoint que irá receber os dados esteja devidamente protegido com um certificado digital emitido por uma *_*Certificate Authority (CA)* reconhecida globalmente, como IndenTrust, DigiCert, Sectigo, GoDaddy, GlobalSign ou Let’s Encrypt.

A variável de TLS `use_tls` recebe `true` ou `false` para ativar/desativar seu uso.

---

### AWS Kinesis Data Firehose

:::note[nota]
Ao usar o endpoint AWS Kinesis Data Firehose, seus eventos são agrupados em blocos de até 500 registros, em vez do padrão de 2000, ou a cada 60 segundos, o que ocorrer primeiro.
:::

Para configurar o endpoint [AWS Kinesis Data Firehose](https://aws.amazon.com/kinesis/data-firehose/), você deve ter acesso à plataforma para obter as seguintes informações:

- **Stream Name**: refere-se ao nome do stream que o usuário definiu ao criar o Kinesis Data Firehose. Exemplo: `MyKDFConnector`_`
- **Region**: refere-se à região para a qual vamos enviar os dados. Exemplo: `us-east-1`.
- **Access Key**: refere-se à chave pública para acessar o Data Firehose, que é fornecida pela AWS. Exemplo: `ORIA5ZEH9MW4NL5OITY4`
- **Secret Key**: refere-se à chave secreta para acessar o Data Firehose, que é fornecida pela AWS. Exemplo: `+PLjkUWJyOLth3anuWXcLLVrMLeiiiThIokaPEiw`

<LinkButton severity="secondary" link="/pt-br/documentacao/produtos/guias/amazon-kinesis-endpoint/" label="Como utilizar o AWS Kinesis Data Firehose para receber dados" />

---

### Azure Blob Storage

Para configurar o endpoint [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs/), você deve ter acesso à plataforma para obter as seguintes informações:

- **Storage Account**: nome da conta de armazenamento que você definiu no Blob Storage. Exemplo: `mystorageaccount`
- **Container Name**: nome do container de armazenamento que você definiu no Blob Storage. Exemplo: `mycontainer`
- **Blob SAS Token**: token gerado pelo Blob Storage. Deve ter acesso concedido para criar, ler, escrever e listar. Exemplo: `sp=oiuwdl&st=2022-04-14T18:05:08Z&se=2026-03-02T02:05:08Z&sv=2020-08-04&sr=c&sig=YUi0TBEt7XTlxXex4Jui%2Fc88h6qAgMmCY4XIXeMvxa0%3F`

<LinkButton severity="secondary" link="/pt-br/documentacao/produtos/guias/azure-blob-endpoint/" label="Como utilizar o Azure Blob Storage para receber dados" />

---

### Azure Monitor

Para configurar o endpoint [Azure Monitor](https://azure.microsoft.com/en-us/products/monitor), você deve ter acesso à plataforma para obter as seguintes informações:

- **Log Type**: o tipo de registro dos dados que estão sendo enviados. Pode conter apenas *letras*, *números*, o *caractere underline (_)* e não pode exceder *100 caracteres*. Exemplo: `AzureMonitorTest`
- **Shared Key**: chave compartilhada do Workspace no Azure Monitor. Exemplo: `OiA9AdGr4As5Iujg5FAHsTWfawxOD4`
- **Time Generated Field**: é usado para gerar o campo TimeGenerated, que traz quanto tempo levará para que o registro fique disponível após ser coletado. Quando não especificado, usa o tempo de ingestão. Exemplo: `myCustomTimeField`
- **Workspace ID**: ID do seu Workspace no Azure Monitor. Exemplo: `kik73154-0426-464c-aij3-eg6d24u87c50`

<LinkButton severity="secondary" link="/pt-br/documentacao/produtos/guias/azure-monitor-endpoint/" label="Como utilizar o Azure Monitor para receber dados" />

---

### Datadog

Para configurar o endpoint [Datadog](https://www.datadoghq.com/), você deve ter acesso à plataforma para obter as seguintes informações:

- **Datadog URL**: a URL ou a URI do seu endpoint no Datadog. Exemplo: `https://inputs.splunk-client.splunkcloud.com:1337/services/collector`
- **API Key**: chave de API gerada no dashboard do Datadog. Exemplo: `ij9076f1ujik17a81f938yhru5g713422`

<LinkButton severity="secondary" link="/pt-br/documentacao/produtos/guias/datadog-endpoint/" label="Como utilizar o Datadog para receber dados" />

---

### Elasticsearch

Para configurar o endpoint [Elasticsearch](https://www.elastic.co/pt/elasticsearch/), você deve ter acesso à plataforma para obter as seguintes informações:

- **Elasticsearch URL**: endereço da URL + index do ElasticSearch que irá receber os dados coletados. Exemplo: `https://elasticsearch-domain.com/myindex`
- **Encoded API Key**: a chave `Base64` correspondente ao valor `"encoded"` gerado na criação da chave API no ElasticSearch. Exemplo: `VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw==`

<LinkButton severity="secondary" link="/pt-br/documentacao/produtos/guias/elasticsearch-endpoint/" label="Como utilizar o Elasticsearch para receber dados" />

---

### Google BigQuery

Para configurar o endpoint [Google BigQuery](https://cloud.google.com/bigquery), você deve ter acesso à plataforma para obter as seguintes informações:

- **Project ID**: ID do seu projeto no Google Cloud. Exemplo: `mycustomGBQproject01`
- **Dataset ID**: ID do seu dataset no Google BigQuery. Ele é único por projeto e diferencia maiúsculas de minúsculas. Exemplo: `myGBQdataset`
- **Table ID**: nome escolhido para a tabela no Google BigQuery. Exemplo: `mypagaviewtable01`
- **Service Account Key**: arquivo JSON fornecido pelo Google Cloud. Ele possui o seguinte formato:

```json
{
    "type": "service_account",
    "project_id": "mycustomGBQproject01",
    "private_key_id": "key-id",
    "private_key": "-----BEGIN PRIVATE KEY-----\nprivate-key\n-----END PRIVATE KEY-----\n",
    "client_email": "service-account-email",
    "client_id": "client-id",
    "auth_uri": "https://accounts.google.com/o/oauth2/auth",
    "token_uri": "https://accounts.google.com/o/oauth2/token",
    "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
    "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/service-account-email"
}
```

<LinkButton severity="secondary" link="/pt-br/documentacao/produtos/guias/google-bigquery-endpoint/" label="Como utilizar o Google BigQuery para receber dados" />

---

### IBM QRadar

Para configurar o endpoint [IBM Qradar](https://www.ibm.com/qradar), você deve ter acesso à plataforma para obter as seguintes informações:

- **URL**: URL configurada em sua plataforma que irá receber os dados.

---

### S3 - Simple Storage Service

:::note
As credenciais S3 utilizadas com o endpoint devem incluir as permissões `List Bucket` (para listar e visualizar os objetos dentro de um bucket) e `Put Object` (para adicionar ou sobrescrever objetos dentro de um bucket), garantindo o acesso necessário para operações de leitura e escrita no armazenamento de informações, incluindo logs. Saiba mais sobre [credenciais S3](/pt-br/documentacao/produtos/store/storage/s3-protocol-para-object-storage/).
:::

Você pode usar qualquer tipo de provedor de S3 (Simple Storage Service) de sua escolha, incluindo o [Object Storage da Azion](/pt-br/documentacao/produtos/store/object-storage/). Para configurar o endpoint S3, você deve ter acesso à plataforma escolhida para obter as seguintes informações:

- **Host URL**: URL do seu Host S3. Exemplo: `https://myownhost.s3.us-east-1.myprovider.com`
    - Ao usar o provedor Amazon S3 Storage, você pode usar o endpoint padrão da AWS: [https://s3.amazonaws.com](https://s3.amazonaws.com/).
- **Bucket Name**: nome do bucket para o qual o objeto será enviado. Exemplo: `mys3bucket`
    - O bucket deve ser criado antes de habilitar o Data Stream a enviar os objetos.
- **Region**: região na qual o bucket está hospedado. Exemplo: `us-east-1`
- **Access Key**: chave pública para acesso ao bucket fornecida pelo seu provedor. Exemplo: `ORIA5ZEH9MW4NL5OITY4`
- **Secret Key**: à chave secreta para acesso ao bucket fornecida pelo seu provedor. Exemplo: `+PLjkUWJyOLth3anuWXcLLVrMLeiiiThIokaPEiw`
- **Object Key Prefix**: prefixo que você pode adicionar aos arquivos que serão enviados. O nome do arquivo é composto pelo Prefixo, pela data e hora em que os pacotes são enviados no formato YYYY/MM/DD/hh/mm/ e pelo UUID. Exemplo: se você usar `user/logs` como prefixo, os objetos enviados serão salvo com a seguinte estrutura `user/logs/2024/10/12/06/24/37d66e78-c308-4006-9d4d-1c013ed89276`.
- **Content Type**: formato em que o objeto será criado no bucket. É possível escolher entre *plain/text* ou *application/gzip*.

<LinkButton severity="secondary"  link="/pt-br/documentacao/produtos/guias/amazon-s3-endpoint/" label="Como utilizar o Amazon S3 para receber dados" />
<LinkButton severity="secondary" link="/pt-br/documentacao/produtos/guias/conector-azion-object-storage/" label="Como utilizar o Object Storage da Azion para receber dados" />

---

### Splunk

Para configurar o endpoint [Splunk](https://www.splunk.com/), você deve ter acesso à plataforma para obter as seguintes informações:

- **Splunk URL**: URL que irá receber os dados coletados. Se você tiver um índice alternativo para indicar, pode adicioná-lo no final da URL. Exemplo: `https://inputs.splunkcloud.com:8080/services/collector?index=myindex`
- **API Key**: token do HTTP Event Collector, fornecido pela instalação do Splunk. Exemplo: `crfe25d2-23j8-48gf-a9ks-6b75w3ska674`

<LinkButton severity="secondary" link="/pt-br/documentacao/produtos/guias/splunk-endpoint/" label="Como utilizar o Splunk para receber dados" />

---

### Standard HTTP/HTTPS POST

:::tip[dica]
Você pode [personalizar o payload](#payload) ao usar esse endpoint.
:::

Para personalizar esse endpoint, você deve obter as seguintes informações:

- **Endpoint URL**: URL que irá receber os dados. Exemplo: `https://app.domain.com/`
- **Custom Headers** (*opcional*): o *nome* e o *valor* para cada cabeçalho para enviar para o endpoint. Você pode informar  um ou mais cabeçalhos customizados para a sua requisição HTTP/HTTPS. Exemplo: `header-name:value`

---

## Payload

Requer: <Tag severity="info" client:only="vue">Endpoint Standard HTTP/HTTPS POST</Tag>

Ao usar o endpoint Standard HTTP/HTTPS POST, o payload para o seu endpoint não é pré-definido. Você tem a opção de personalizar as informações essenciais que serão enviadas em seus dados da forma que achar melhor.

Para personalizar o payload enviado pelo conector no **Data Stream**, você tem os seguintes campos:

- **Max Size** (*opcional*): define, em bytes, o tamanho de pacotes de dados que serão enviados. Aceita valores a partir de *1000000*.
- **Log Line Separator** (*opcional*): define que informação será usada no fim de cada linha de registro (log). Usado para quebrar informações em linhas diferentes.
- **Payload Format** (*opcional*): define qual informação será enviada em seus dados para cada requisição de Data Stream.

Por padrão, o Data Stream recomenda o formato NDJSON com o uso de `\n` como um log line separator e $dataset como payload format, que usa as informações fornecidas no campo de código **Data Set**, descrevendo as variáveis escolhidas como template.

Você pode escolher outras opções para ambos os campos. Dependendo dos tipos de seus registros, você pode usar `,` como log line separator, por exemplo.

O formato NDJSON não usa os arrays típicos de JSON `[]`, e cada dado é apresentado em uma linha diferente, sem vírgula separando-as. Ele pode ser útil para dados estruturados com processamentos de um registro por vez. Já o formato JSON é o mais conhecido e possui tabulação de dados, usando arrays `[]` e sendo separado por vírgulas. Com ele, o payload pode ser tratado como um registro individual.

Por exemplo, se o **Log Line Separator** receber `,` e o **Payload Format** receber `[$dataset]`, e o template tem as seguintes variáveis no campo de código **Data Set**:

$request_method\
$host\
$status

Você receberá uma resposta em JSON semelhante a isso:

```json
    [
        {
            "request_method": "GET",
            "host": "www.onedomain.com",
          "status": "200"
        },
        {
            "request_method": "POST",
            "host": "www.anotherdomain.com.br",
            "status": "200"
        }
    ]
```

Mas se o **Log Line Separator** receber `\n` e o **Payload Format** receber `$dataset`, você receberá uma resposta em NDJSON semelhante a isso:

```json
{"request_method": "GET", "host": "www.onedomain.com", "status": "200"}
{"request_method": "POST", "host": "www.anotherdomain.com.br", "status": "200"}
```

### Payload personalizado com Activity History

Você pode personalizar o payload com informações específicas se você está usando o data source [Activity History](#activity-history) e o endpoint [Standard HTTP/HTTPS POST](#standard-http-https-post).

As seguintes informações devem ser usadas nos campos de payload para configurá-lo:

- **Log Line Separator**: \n
- **Payload Format**: 'v1\t$time_iso8601\t$clientid\t$title\t$comment\t$type\t$author_name\t$author_email'

---

## Tratamento de erros

Os servidores do **Data Stream** funcionam em duas etapas: monitoram os endpoints uma vez por minuto (1x/min) e declaram o endpoint como *available*, disponível, ou *unavailable*, indisponível.

O Data Stream tenta enviar mensagens para endpoints que estão *available*. Se o endpoint estiver *unavailable*, as mensagens não são enviadas, pois as informações são descartadas. No minuto seguinte, o Data Stream faz o envio dos dados novamente se o endpoint ficar *available*.

Para ser considerado como *available*, o endpoint deve ser aprovado por todos os servidores da Azion. Se um dos servidores indicar que o endpoint está *unavailable*, as mensagens não são enviadas.

### Comportamento do status HTTP 504

Ocorre quando o endpoint responde ao teste com sucesso, mas não recebe as mensagens dentro do tempo de timeout: *20 segundos* para endpoints do tipo HTTP POST.

### Comportamento do status HTTP 503

Ocorre quando o endpoint é declarado como *unavailable*, indisponível, pela monitoração do endpoint. Portanto, o Data Stream não tenta enviar as mensagens.

Como o sistema é distribuído, não é possível saber qual o servidor específico que envia as mensagens para cada endpoint.

---

## Limites

:::tip
**Aumente limites** <br></br>
Você pode solicitar o aumento dos limites com base no seu plano. Contate o [time de suporte técnico](/pt-br/documentacao/servicos/suporte/) para fazer a solicitação.
:::

Estes são os **limites default**:

| Recurso | Limite |
| ------- | ------ |
| Data Stream | 60 segundos ou pacotes de 2.000 requisições, o que ocorrer primeiro.<br/ ><br />Para o endpoint **AWS Kinesis Data Firehose**: 60 segundos ou pacotes de 500 requisições |
| Sampling | 1 stream ativo |
| Tempo para os dados serem transferidos para infraestrutura do cliente | Até 3 minutos |

<br /><br />
---


